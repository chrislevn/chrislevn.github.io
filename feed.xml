<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://christopherle.com//feed.xml" rel="self" type="application/atom+xml"/><link href="https://christopherle.com//" rel="alternate" type="text/html" hreflang="en"/><updated>2024-02-13T00:46:48+00:00</updated><id>https://christopherle.com//feed.xml</id><title type="html">blank</title><subtitle>A software engineer with a passion for connecting all things infrastructure and data. </subtitle><entry><title type="html">My thought process building end-to-end low-cost backend</title><link href="https://christopherle.com//blog/2024/thought-process-project/" rel="alternate" type="text/html" title="My thought process building end-to-end low-cost backend"/><published>2024-02-12T23:35:00+00:00</published><updated>2024-02-12T23:35:00+00:00</updated><id>https://christopherle.com//blog/2024/thought-process-project</id><content type="html" xml:base="https://christopherle.com//blog/2024/thought-process-project/"><![CDATA[<h2 id="research">Research</h2> <p>FYI, I haven’t done any real production backend yet nor I had a chance to. But I was determined to try, research, and learn about it. The first thing I learned was</p> <blockquote> <p>Software engineer is a constant progress of problem solving.</p> </blockquote> <p><img src="https://cdn2.hubspot.net/hubfs/1751195/blg-img.jpg" alt="Source: Pixentia Insights"/></p> <p>There is no such thing as built once and done. There will be maintaince, bug fixing, new features or optimization. I learned this from an older friend who is a tech lead for 5+ years. That said, to have the most real experience, finding someone who need their software to be built and help them build it is the best way to learn. Alternatively, I also thought of building my own personal project but it has to be released to the public and maintained for a while.</p> <p>Luckily, after I found one, the first thing I researched was what was the business needs:</p> <ul> <li>How big is the company? What is the priority? Do they need to scale or just need to be built fast?</li> </ul> <p>The current boxes were: something cheap or even free until we have more profits, something that can be built fast, and something that can be scaled and customized. Then, I aligned this with what I already knew.</p> <h3 id="tech-stacks">Tech stacks</h3> <p><img src="https://novateus.com/blog/wp-content/uploads/2022/03/yvcl5it66j0uni2xz8tc.png" alt="Source: Novateus"/></p> <p>Starting point: I have experience in Full-stack, DevOps, and Machine Learning. However, unlike DevOps, most of Full-stack and Machine Learning products I built was stopped at the prototype stage. I want to learn the best practices and how to build a real product that fit business needs.</p> <p>Learning resources I used:</p> <ul> <li><a href="https://roadmap.sh/backend">Backend roadmap</a></li> <li><a href="https://roadmap.sh/best-practices/api-security">API Security</a></li> <li>System design interviews pt1&amp;2 book.</li> <li>Design data-intensive applications book.</li> <li>Site Reliability Engineering from Google.</li> <li>Python for DevOps book.</li> <li>High Performance Python book.</li> <li>Robust Python book for clean and maintainable code.</li> <li>Mutiple documentations.</li> </ul> <p>Let’s get started.</p> <ul> <li> <p>I’m strongest in Python since I also do data engineering, which made development time faster. I do understand Go, JavaScript is better backend choice but there will make the development time longer.</p> <p>=&gt; I chose <strong>Python</strong>. For framework, I chose <strong>Flask</strong> due to its simplicity and support for both front-end and back-end. Will update to FastAPI later for better performance. At the moment, great performance is not the priority since user base is small. I find sometimes I have to link Python with JavaScript since the company also has a front-end team. Learning JavaScript is useful for this case.</p> </li> <li> <p>I have experience in full-stack and devops. The next things was finding what fits the budget. Use “back-of-the-envelope” calculation to estimate the cost, server need:</p> <ul> <li> <p>SQL or NoSQL: I found SQL is better and faster for query and data manipulation. But currently, the data is small and NoSQL databases out there aren’t free. For SQL, I found AWS DynamoDB offer the most for free tier (25GB). After a while, I found it’s quite stable and fast. They also have CloudWatch to monitor database performance but please turn them off if you don’t need it to save cost. Once we have more profits, we can move to SQL. =&gt; Choice: NoSQL with AWS DynamoDB.</p> </li> <li> <p>Hosting: I currently use <strong>Render</strong> free tier since they have very easy deployment with Flask. However, their free tier server will sleep unexpectedly after some inactivity. However, EC2 is the best choice with 12 months free tier and it is stable. CI/CD deployment with EC2 however is not easy. They requires a bunch of configurations and fixing bugs (devops) will be a pain for those who are not familiar with it once I left this project to someone else. =&gt; Choice: <strong>Render</strong> for now. The goal is to move to EC2 once working MVP is done.</p> </li> </ul> <blockquote> <p>For server, profiling and load testing your software will help determine how much RAM you will need. For CPU, the more CPU will help with the better performance.</p> </blockquote> <ul> <li> <p>Email service: I found <strong>AWS SES</strong> is the best for free tier for my need. It’s stable and fast. They also offer customization HTML. The email verification email, however, needs production access from AWS, which requires some communication with the company.</p> </li> <li> <p>CI/CD: I found <strong>GitHub Actions</strong> is the best for this case since the company is small and the deployment is simple. Its free tier is also enough for this case.</p> </li> </ul> </li> </ul> <p>That’s all the simple needs for the backend. We will talk more about the customizations and scaling later.</p> <h2 id="development">Development</h2> <p><img src="https://miro.medium.com/v2/resize:fit:1358/0*qZ1JOnVpPokvnr69.jpg" alt="Source: Synotive"/></p> <ol> <li>The first thing I would do is to confirm the architecture, tech stack, and database schema with the tech lead/CTO/tech manager. Then, estimate the deadline for each features and also collaborate with non-tech managers to align the deadline with the business needs. This whole process will take around 1-2 weeks. Think of it from user perpective on how they would use this product. Raise the questions “what if?” to cover test cases.</li> </ol> <blockquote> <p>Remember that not all businesses will need a lot of technology. Be flexible and aim for solving the problems.</p> </blockquote> <p>An example can be from how you create user id to will read or write will be more frequent? What kinds of data type and data structure will be used? What kind of database will need TTL (time to live)? Do we need to use cache? (my case yes, it was a lot faster).</p> <ol> <li>Then, I follow the deadlines to build small features. Anything big can be achieved by breaking it down to smaller tasks. I used Github Issues and Projects to manage the tasks.</li> </ol> <ul> <li>I tested interacting with databases, sending emails on local, etc.</li> <li>Then, I figured to deploy the server.</li> <li>Then, I configured the Github Actions and API to manage the flows.</li> <li>Once I tested the product myself fully, I asked tech people in the team to first test it to see errors I couldn’t see myself.</li> <li>Once all is satisfied, I tested with non-tech people to see if the product is easy to use for first time users.</li> </ul> <blockquote> <p>I found Copilot is super helpful to make deployment and testing faster. However, supervision is needed since it can make mistakes.</p> </blockquote> <blockquote> <p>The flow above looks simple but what took me longest was making sure all the edge cases was covered and the product is easy to use to fit the business needs. That comes to optimization.</p> </blockquote> <h2 id="optimization">Optimization</h2> <p><img src="https://www.baeldung.com/wp-content/uploads/sites/4/2022/08/levels4-e1661327598293.png" alt="Source: Baeldung"/> SWE is not a deployed and done. It’s a constant progress. I found that the most important thing is to make sure the product is easy to use and fast. An example can be if the user has to wait 5s for the server to respond, they will leave.</p> <p>I found this server slowness comes from a few reasons. And also there are some questions I asked myself to improve the product:</p> <ul> <li>Server sleeps. Is there any way we can prevent this?</li> <li>Lookup time is slow. Can we cache the data? or improve the algorithm? <blockquote> <p>An example is if we want to find the customer with most orders and there are a lot of finding and customers adding, we can use a heap?</p> </blockquote> </li> <li>Server has “503 error” when deploying. Can we stage the deployment with dev, staging, and production with different servers? If we have test server, do we have authentication so only the team can access it?</li> <li>How can we prevent this from happening again? Do we have monitoring tools? Did we do profiling and performance testing?</li> <li>For scaling, what are our SLOs (service level objectives)? What are our SLAs (service level agreements)? What are our KPIs (key performance indicators)? How do we measure them? and how do we improve them?</li> <li>Testing will other people always have bugs. Did we have a good testing practices before deploying?</li> <li>Did we have A/B testing? Did we have a way to rollback if the new feature is not working?</li> <li>Did we have a way to alert the team if the server is down? or if the automation is not working?</li> <li>Did we handle null, repeated sent emails, or other edge cases?</li> <li>Did we have type checking and error handling?</li> <li>Do we have rate limiting to avoid spamming and DDoS attacks?</li> <li>How do we handle logs and tracing?</li> <li>Is our API secure? Did we use JWT? Did we use HTTPS? Did we use CORS?</li> <li>Did we make automation and documentation for the next person who will take over the project?</li> </ul> <blockquote> <p>It might sound a lot but it do get better and you will learn a lot from it. I found the term <strong>10x engineer</strong> is also related to this? In a meeting with non-tech people, the <strong>10x engineer</strong> already tested their software throughly, can answer questions, and add features that might be useful later. That way, they finish the project faster and can move to the next project.</p> </blockquote> <blockquote> <p>Don’t forget to prioritize the optimization. My approach was always keep 3 most important things on my current todos. In this case, I purposely overengineered the product to learn more about the best practices. Things like optimize the performance, security, and automation will be useful in the future. However, I also made it easy for the next person to take over with documentation, automation, and tracing appraoches.</p> </blockquote> <p>Back to optimization with Python, here are the tools I used:</p> <ul> <li>Testing: Pytest</li> <li>Profiling: cProfile with SnakeViz</li> <li>Monitoring: Sentry or Flask_monitoringdashboard (FMD is quite buggy and not recommended for production). Grafana and Prometheus are also good but they require a lot of configurations, espcially DevOps, will be used in the future.</li> <li>Load testing: Locust</li> <li>Tracing: Sentry</li> <li>Automation: GitHub Actions</li> <li>Rate limiting: Flask-Limiter</li> <li>Logs: logger from loguru</li> <li>Business visualization: AWS CloudWatch. In the future, I will use Grafana.</li> <li>Documentation: Sphinx or FastAPI/docs</li> <li>Authentication: this part, I recommend Auth0 for its simplicity and security.</li> </ul> <p>All of this, I made Makefile automation to make it easier for the next person to take over.</p> <blockquote> <p>Once the product is up and running, the optimization can be started from a very small feature (like making a function faster and consume less memory) to a big feature (like refractoring the whole code base). This deep knowledge is hardly taught in bootcamps but colleges (data structures, hardware, computer systems, etc). Constant learning and experience is needed.</p> </blockquote> <blockquote> <p>Tech stack in this case is neccessary but not the most important. Understanding the business needs, users flow, foundations of the tech stack, practices to find solutions are what kept me going. But if I have to do more to optimize, I will do this:</p> </blockquote> <h3 id="future-development">Future development</h3> <ul> <li>Monitoring: Grafana and Prometheus</li> <li>Server: EC2</li> <li>Database: SQL</li> <li>Upgrade Github Team to have merge check and staging environment.</li> <li>Use FastAPI for better performance.</li> <li>Maybe migrate to Rust/Go server for better performance? (question mark since this might mean refractoring the whole code base and the team might not have the experience with it.)</li> </ul> <h2 id="finish">Finish</h2> <p><img src="https://media.istockphoto.com/id/1392694760/vector/working-project-progress-effort-to-finish-work-or-achieve-business-success-accomplishment.jpg?s=612x612&amp;w=0&amp;k=20&amp;c=HTNI6ZqgVCBs7KVzt6IwiD7APe2ckyxazh9RWYMX7Zc=" alt="Source: iStock"/> At the time I wrote this post, the project is done or might be tested for the release. The current cost (for all the open-source tools I used) is nearly $0 for +250,000 emails sent/year (more observation will be needed in the future). I followed all the best practices from <a href="https://roadmap.sh/backend">Backend roadmap</a>. Looking back, I can see this is more than just reading as I can see and apply what I learned. I could also see what happens if I didn’t do this or that.</p> <p>Things can be from how to build this product to what is the pros and cons of each tech stack, even at the language level. I got to test the performance of Python and Go, Rust server because I was genuinely curious.</p> <p>From my observations, junior developers can make things perfectly if there are instructions. But when things aren’t clear, more seniority is needed. An example would be a junior developer can learn everything about system design from books and interviews practices but a senior developer will know the pros and cons and if it it’s suitable for the business. That is where the <em>experience</em> comes in. Can’t trade it with AI.</p> <p>After this project, I have more confidence to step in bigger code bases. For those who are learning and wonder how to get good in tech, here is my recomendation:</p> <ul> <li>While at school, learn the basics well since you will need it later. All the new frameworks are based on the basics. Knowing the basics will help you know the pros and cons and how to improve the solutions.</li> <li>Find a problem or someone who needs a software and help them build it. Get it done is not easy but you will get the foundation of how to build a software, espcially with others and for others.</li> <li>Read and apply. Reading is not enough. You need to apply it to see if it’s true. Learning never stops.</li> <li>Be curious. Always ask “why” and “what if”.</li> <li>Once you feel comfortable, step out of your comfort zone: find an internship, reasearch project, job, or collaborate on an open-source project. The point is to learn from others and see how they do things. Learn to use Git properly with a large team.</li> <li>Duirng doing these, learn from your mentors and peers. They can be the managers or the people you work with. They can be the people you meet at meetups or online.</li> </ul> <blockquote> <p>Remember, SWE is not just coding. Sometimes, to solve the problem, you need to talk to people, understand the business, and understand the users. The end goal is to solve the problem as the core of engineering. In the real world with competiton, solving problem will also involve solving it faster and better.</p> </blockquote> <h2 id="my-take-on-ai-and-my-learning">My take on AI and my learning</h2> <p><img src="https://cdn-blog.scalablepath.com/uploads/2022/09/github-copilot-pair-programmer-copy.png" alt="Source: Scalable Path"/> Current LLM is far from AGI aka human doing nothing. But it’s undeniable we got more done with less resouces (one senior developer with AI compare with the cost of 1 senior, 2 junior developers). Since training AI will also be costly, cutting costs and laying off is inevitable. Of course, there will be jobs for AI but it will take time for the market to adjust and more barriers to entry (Masters and PhD to get a role in AI on average). But AI is a must for the future. Looking back on human development, technlogy will advance and we will have to adapt.</p> <p>That’s why I feel the immediate needs to upskill of what I already know and also broaden my skills with AI. The question is at the end, if nobody will hire me, can I build my own product? What skills do I need? What people do I need to get in my team? Or if say opening a company is too hard, can I freelance and build products for others? What tech and non-tech skills do I need? Experience is so important in this case.</p> <p>When I had less real-life experience, my tech stack came from big names like Google (Firebase, GCP, etc) and AWS. They all have free tier to encourage people to use their products. I made poor code and optimization because everything was free. But when I had more experience, I learned more about the pros and cons of each product. For example, can Firebase, which was free and easy to use, handle the load of a big company? Can AWS, which was more complex and costly, be used for a small company?</p> <blockquote> <p>I found that in a company. CEO is often more optimist and CTO is often more pessimist. Optimism from the CEO helps sales, and calls for more investment. Pessimism from the CTO helps determine the cost and deployment of the tech stacks. Both are important and reliable on each other.</p> </blockquote> <p>I also learned about the importance of the community. An example is if you have a problem, you can ask the community. Not everything can be answered from StackOverflow, Medium, or AI (ChatGPT). You might have to contact the person who wrote the code and ask them or anyone related. Communication is not hidden for SWEs.</p> <h2 id="conclusion">Conclusion</h2> <p>I hope this post is helpful for those who are learning and those who are curious about how to build a backend. Stay tuned! I’m starting on a new cool project. I will write more about my learning in the next posts.</p> <p>If you want to collaborate or hire me to solve your problems, please contact me at <a href="mailto:locvicvn1234@gmail.com">locvicvn1234@gmail.com</a>. I’m also open to any feedback and questions.</p>]]></content><author><name></name></author><category term="backend"/><category term="languages"/><category term="python"/><category term="algorithms"/><category term="backend"/><summary type="html"><![CDATA[Research]]></summary></entry><entry><title type="html">Good practices on writing Dockerfile</title><link href="https://christopherle.com//blog/2023/dockerfile-practices/" rel="alternate" type="text/html" title="Good practices on writing Dockerfile"/><published>2023-11-12T23:35:00+00:00</published><updated>2023-11-12T23:35:00+00:00</updated><id>https://christopherle.com//blog/2023/dockerfile-practices</id><content type="html" xml:base="https://christopherle.com//blog/2023/dockerfile-practices/"><![CDATA[ <p>Website: <a href="https://christopherle.com/blog/2023/dockerfile-practices/#s2-dockerfiles-practices">https://christopherle.com/blog/2023/dockerfile-practices/#s2-dockerfiles-practices</a> <br/> Github: <a href="https://github.com/chrislevn/dockerfile-practices">https://github.com/chrislevn/dockerfile-practices</a> <br/> Simple demos running Docker with Python: <a href="https://github.com/chrislevn/dockerfile-practices/tree/main/demo">https://github.com/chrislevn/dockerfile-practices/tree/main/demo</a></p> <details> <summary>Table of Contents</summary> <ul> <li><a href="#s1-background">1 Background</a></li> <li><a href="#s2-dockerfiles-practices">2 Dockerfile’s practices</a> <ul> <li><a href="#s2.1-use-minimal-base-images">2.1 Use minimal base images</a></li> <li><a href="#s2.2-use-explicit-tags-for-the-base-image">2.2 Use explicit tags for the base image.</a></li> <li><a href="#s2.3-leverage-layer-caching">2.3 Leverage layer caching</a></li> <li><a href="#2.4-consolidate-related-operations">2.4 Consolidate related operations</a></li> <li><a href="#s2.5-remove-unnecessary-artifacts">2.5 Remove unnecessary artifacts</a></li> <li><a href="#s2.6-use-specific-copy-instructions">2.6 Use specific COPY instructions</a></li> <li><a href="#s2.7-document-your-dockerfile">2.7 Document your Dockerfile</a></li> <li><a href="#s2.8-use-dockerignore-file">2.8 Use .dockerignore file</a></li> <li><a href="#s2.9-test-your-image">2.9 Test your image</a></li> <li><a href="#s2.10-add-or-copy">2.10 ADD or COPY</a></li> </ul> </li> <li><a href="#s3-security-practices">3 Security practices:</a> <ul> <li><a href="#s3.1-use-environment-variables-for-configuration">3.1 Use environment variables for configuration</a> <ul> <li><a href="#s3.1.1-setting-dynamic-environment-values-arg-vs-env">3.1.1 Setting Dynamic Environment Values (ARG vs ENV)</a></li> </ul> </li> <li><a href="#s3.3-set-the-correct-container-user">3.3 Set the correct container user</a></li> <li><a href="#s3.4-create-a-non-root-user-in-the-dockerfile">3.4 Create a non-root user in the Dockerfile</a></li> <li><a href="#s3.5-avoid-running-containers-with-root-privileges">3.5 Avoid running containers with root privileges</a></li> </ul> </li> <li><a href="#s4-other-references">4 Other references:</a> <ul> <li><a href="#s4.1-expose">4.1 EXPOSE</a></li> <li><a href="#s4.2-entrypoint-vs-cmd-vs-run">4.2 ENTRYPOINT vs CMD vs RUN</a></li> <li><a href="#s4.3-docker-image-vs-docker-containers">4.3 Docker Image vs Docker Containers:</a> <ul> <li><a href="#s4.3.1-docker-image">4.3.1 Docker Image:</a></li> <li><a href="#s4.3.2-docker-container">4.3.2 Docker Container:</a></li> <li><a href="#s4.3.3-docker-image-vs-containers">4.3.3 Docker Image vs Containers</a></li> </ul> </li> <li><a href="#s4.4-workdir">4.4 WORKDIR</a></li> <li><a href="#s4.5-volume">4.5 VOLUME</a></li> <li><a href="#s4.6-user">4.6 USER</a></li> <li><a href="#s4.7-onbuild">4.7 ONBUILD</a></li> </ul> </li> <li><a href="#s5-good-demo">5 Good demo</a></li> <li><a href="#s6-basic-steps-to-running-docker-file-with-docker-cli">6 Basic steps to running Docker file with docker cli:</a> <ul> <li><a href="#s6.1-run-docker-with-docker-compose">6.1 Running Docker with Docker compose</a> <ul> <li><a href="#s6.1.1-docker-compose">6.1.1 Docker compose</a></li> <li><a href="#s6.1.2-docker-compose-vs-docker-run&quot;">6.1.2 Docker compose vs Docker run</a></li> <li><a href="#s6.1.3-docker-compose-usage">6.1.3 Docker compose usage</a></li> </ul> </li> <li><a href="#s6.2-run-docker-with-kubernetes">6.2 Running Docker image with Kubernetes</a> <ul> <li><a href="#s6.2.1-what-is-kubernetes">6.2.1 What is Kubernetes</a></li> <li><a href="#s6.2.2-run-public-docker-image-with-kubernetes">6.2.2 Running public Docker image with Kubernetes</a></li> <li><a href="#s6.2.3-run-private-docker-image-with-kubernetes">6.2.3 Running private Docker image with Kubernetes</a></li> <li><a href="#s6.2.4-run-docker-image-with-minikube">6.2.4 Running Docker image with Minikube</a></li> </ul> </li> <li><a href="#s6.3-add-load-balancer-with-nginx">6.3 Add load balancer with Nginx</a> <ul> <li><a href="#s6.3.1-what-is-load-balancer">6.3.1 What is load balancer</a></li> <li><a href="#s6.3.2-ngnix">6.3.2 Ngnix</a></li> <li><a href="#s6.3.3-add-load-balancer-to-docker-compose">6.3.3 Add load balancer to Docker compose</a></li> </ul> </li> </ul> </li> <li><a href="#s7-contributing">7 Contributing</a> <ul> <li><a href="#s7.1-contributing-guide">7.1 Contributing guide</a></li> <li><a href="#s7.2-acknowledgement">7.2 Acknowledgement</a></li> </ul> </li> <li><a href="#s8-references">8 References:</a></li> </ul> </details> <hr/> <h2 id="1-background">1. Background</h2> <p><a id="s1-background"></a></p> <p>Docker is an open-source platform that enables you to automate the deployment, scaling, and management of applications using containerization. It provides a way to package applications and their dependencies into a standardized unit called a container.</p> <p>This guide is a list of practices I have collected, while learning Docker, for building your own Dockerfile. If you have new tips, feel free to contribute via <a href="https://github.com/chrislevn/dockerfile-practices/blob/main/CONTRIBUTING.md">Contributing guide</a>. Hope this helps!</p> <h2 id="2-dockerfiles-practices">2. Dockerfile’s practices</h2> <p><a id="s2-dockerfiles-practices"></a></p> <h3 id="21-use-minimal-base-images">2.1 Use minimal base images</h3> <p><a id="s2.1-use-minimal-base-images"></a></p> <p>Start with a minimal base image that contains only the necessary dependencies for your application. Using a smaller image reduces the image size and improves startup time.</p> <p>No:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> python:3.9</span>
</code></pre></div></div> <p>Yes:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> python:3.9-slim</span>
</code></pre></div></div> <p>Base image types:</p> <ul> <li>stretch/buster/jessie: is the codename for Debian 10, which is a specific version of the Debian operating system. Debian-based images often have different releases named after characters from the Toy Story movies. For example, “Jessie” refers to Debian 8, “Stretch” refers to Debian 9, and “Buster” refers to Debian 10. These releases represent different versions of the Debian distribution and come with their own set of package versions and features.</li> <li>slim: is a term commonly used to refer to Debian-based base images that have been optimized for size. These images are built on Debian but are trimmed down to include only the essential packages required to run applications. They are a good compromise between size and functionality, providing a balance between a minimal footprint and the availability of a wide range of packages.</li> <li>alpine: Alpine Linux is a lightweight Linux distribution designed for security, simplicity, and resource efficiency. Alpine-based images are known for their small size and are highly popular in the Docker ecosystem. They use the musl libc library instead of the more common glibc found in most Linux distributions. Alpine images are often significantly smaller compared to their Debian counterparts but may have a slightly different package ecosystem and may require adjustments to some application configurations due to the differences in the underlying system libraries. However, some teams are moving away from alpine because these images can cause compatibility issues that are hard to debug. Specifically, if using python images, some wheels are built to be compatible with Debian and will need to be recompiled to work with an Apline-based image.</li> </ul> <p>References:</p> <ul> <li>https://medium.com/swlh/alpine-slim-stretch-buster-jessie-bullseye-bookworm-what-are-the-differences-in-docker-62171ed4531d</li> </ul> <h3 id="22-use-explicit-tags-for-the-base-image">2.2 Use explicit tags for the base image.</h3> <p><a id="s2.2-use-explicit-tags-for-the-base-image"></a></p> <p>Use explicit tags for the base image instead of generic ones like ‘latest’ to ensure the same base image is used consistently across different environments.</p> <p>No:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> company/image_name:latest</span>
</code></pre></div></div> <p>Yes:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> company/image_name:version</span>
</code></pre></div></div> <h3 id="23-leverage-layer-caching">2.3 Leverage layer caching</h3> <p><a id="s2.3-leverage-layer-caching"></a></p> <p>Docker builds images using a layered approach, and it caches each layer. Place the instructions that change less frequently towards the top of the Dockerfile. This allows Docker to reuse cached layers during subsequent builds, speeding up the build process.</p> <p>No:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Install system dependencies</span>
<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> <span class="se">\
</span>    apt-get <span class="nb">install</span> <span class="nt">-y</span> curl

<span class="c"># Install application dependencies</span>
<span class="k">RUN </span>curl <span class="nt">-sL</span> https://deb.nodesource.com/setup_14.x | bash - <span class="o">&amp;&amp;</span> <span class="se">\
</span>    apt-get <span class="nb">install</span> <span class="nt">-y</span> nodejs

<span class="c"># Copy application files</span>
<span class="k">COPY</span><span class="s"> . /app</span>

<span class="c"># Build the application</span>
<span class="k">RUN </span><span class="nb">cd</span> /app <span class="o">&amp;&amp;</span> <span class="se">\
</span>    npm <span class="nb">install</span> <span class="o">&amp;&amp;</span> <span class="se">\
</span>    npm run build

</code></pre></div></div> <p>In this example, each <code class="language-plaintext highlighter-rouge">RUN</code> instruction creates a new layer, making it difficult to leverage layer caching effectively. Even if there are no changes to the application code, every step from installing system dependencies to building the application will be repeated during each build, resulting in slower build times.</p> <p>Yes:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Install system dependencies</span>
<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> <span class="se">\
</span>    apt-get <span class="nb">install</span> <span class="nt">-y</span> curl

<span class="c"># Install application dependencies</span>
<span class="k">RUN </span>curl <span class="nt">-sL</span> https://deb.nodesource.com/setup_14.x | bash - <span class="o">&amp;&amp;</span> <span class="se">\
</span>    apt-get <span class="nb">install</span> <span class="nt">-y</span> nodejs

<span class="c"># Set working directory</span>
<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="c"># Copy only package.json and package-lock.json</span>
<span class="k">COPY</span><span class="s"> package.json package-lock.json /app/</span>

<span class="c"># Install dependencies</span>
<span class="k">RUN </span>npm <span class="nb">install</span>

<span class="c"># Copy the rest of the application files</span>
<span class="k">COPY</span><span class="s"> . /app</span>

<span class="c"># Build the application</span>
<span class="k">RUN </span>npm run build
</code></pre></div></div> <p>In this improved example, we take advantage of layer caching by separating the steps that change less frequently from the steps that change more frequently. Only the necessary files (package.json and package-lock.json) are copied in a separate layer to install the dependencies. This allows Docker to reuse the cached layer for subsequent builds as long as the dependency files remain unchanged. The rest of the application files are copied in a separate step, reducing unnecessary cache invalidation.</p> <h3 id="24-consolidate-related-operations">2.4 Consolidate related operations</h3> <p><a id="2.4-consolidate-related-operations"></a></p> <p>Minimize the number of layers by combining related operations into a single instruction. For example, instead of installing multiple packages in separate <code class="language-plaintext highlighter-rouge">RUN</code> instructions, group them together using a single RUN instruction.</p> <p>No:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Install dependencies</span>
<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> <span class="se">\
</span>    apt-get <span class="nb">install</span> <span class="nt">-y</span> curl

<span class="c"># Install Node.js</span>
<span class="k">RUN </span>curl <span class="nt">-sL</span> https://deb.nodesource.com/setup_14.x | bash - <span class="o">&amp;&amp;</span> <span class="se">\
</span>    apt-get <span class="nb">install</span> <span class="nt">-y</span> nodejs

<span class="c"># Install project dependencies</span>
<span class="k">RUN </span>npm <span class="nb">install </span>express
<span class="k">RUN </span>npm <span class="nb">install </span>lodash
<span class="k">RUN </span>npm <span class="nb">install </span>axios

</code></pre></div></div> <p>In this example, each package installation is done in a separate <code class="language-plaintext highlighter-rouge">RUN</code> instruction. This approach creates unnecessary layers and increases the number of cache invalidations. Even if one package changes, all subsequent package installations will be repeated during each build, leading to slower build times.</p> <p>Yes:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Install dependencies and Node.js</span>
<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> <span class="se">\
</span>    apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="se">\
</span>        curl <span class="se">\
</span>        nodejs

<span class="c"># Install project dependencies</span>
<span class="k">RUN </span>npm <span class="nb">install </span>express lodash axios
</code></pre></div></div> <p>In this improved example, related package installations are consolidated into a single RUN instruction. This approach reduces the number of layers and improves layer caching. If no changes occur in the package.json file, Docker can reuse the previously cached layer for the npm install step, resulting in faster builds.</p> <h3 id="25-remove-unnecessary-artifacts">2.5 Remove unnecessary artifacts</h3> <p><a id="s2.5-remove-unnecessary-artifacts"></a></p> <p>Clean up any unnecessary artifacts created during the build process to reduce the size of the final image. For example, remove temporary files, unused dependencies, and package caches.</p> <p>No:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Install dependencies</span>
<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> <span class="se">\
</span>    apt-get <span class="nb">install</span> <span class="nt">-y</span> curl

<span class="c"># Download application package</span>
<span class="k">RUN </span>curl <span class="nt">-O</span> https://example.com/app.tar.gz

<span class="c"># Extract application package</span>
<span class="k">RUN </span><span class="nb">tar</span> <span class="nt">-xzf</span> app.tar.gz

<span class="c"># Remove unnecessary artifacts</span>
<span class="k">RUN </span><span class="nb">rm </span>app.tar.gz
</code></pre></div></div> <p>In this example, the unnecessary artifacts, such as the downloaded app.tar.gz file, are removed in a separate RUN instruction. However, this approach doesn’t take advantage of Docker’s layer caching. Even if no changes are made to the downloaded package, Docker will not be able to reuse the cached layer and will repeat the download, extraction, and removal steps during each build.</p> <p>Yes:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Install dependencies</span>
<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> <span class="se">\
</span>    apt-get <span class="nb">install</span> <span class="nt">-y</span> curl

<span class="c"># Download and extract application package, then remove unnecessary artifacts</span>
<span class="k">RUN </span>curl <span class="nt">-O</span> https://example.com/app.tar.gz <span class="se">\
</span>    <span class="o">&amp;&amp;</span> <span class="nb">tar</span> <span class="nt">-xzf</span> app.tar.gz <span class="se">\
</span>    <span class="o">&amp;&amp;</span> <span class="nb">rm </span>app.tar.gz
</code></pre></div></div> <p>In this improved example, the unnecessary artifacts are removed immediately after they are no longer needed, within the same <code class="language-plaintext highlighter-rouge">RUN</code> instruction. By doing so, Docker can leverage layer caching effectively. If the downloaded package remains unchanged, Docker can reuse the cached layer, avoiding redundant downloads and extractions.</p> <p>In Python, avoid using <code class="language-plaintext highlighter-rouge">pip freeze &gt; requirements.txt</code> to generate libaries as it will install all related packages which can cause bugs, conflicts, and big file size.</p> <p>Solutions:</p> <ul> <li>Use <code class="language-plaintext highlighter-rouge">pipreqs</code>: pipreqs starts by scanning all the python files (.py) in your project, then generates the requirements.txt file based on the import statements in each python file of the project.</li> </ul> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">pip install pipreqs
</span></code></pre></div></div> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">pipreqs /&lt;your_project_root_path&gt;</span>/
</code></pre></div></div> <p>Sometimes you might want to update the requirement file. In this case, you need to use the –forceoption to force the regeneration of the file.</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">pipreqs --force /&lt;your_project_root_path&gt;</span>/
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">cat requirements.txt | xargs -n 1 pip install</code> <br/> Note: -a parameter is not available under MacOS, so old cat is more portable. Reference: <a href="https://stackoverflow.com/questions/22250483/stop-pip-from-failing-on-single-package-when-installing-with-requirements-txt">https://stackoverflow.com/questions/22250483/stop-pip-from-failing-on-single-package-when-installing-with-requirements-txt</a></li> </ul> <h3 id="26-use-specific-copy-instructions">2.6 Use specific COPY instructions</h3> <p><a id="s2.6-use-specific-copy-instructions"></a></p> <p>When copying files into the image, be specific about what you’re copying. Avoid using . (dot) as the source directory, as it can inadvertently include unwanted files. Instead, explicitly specify the files or directories you need.</p> <p>No:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Copy all files into the image</span>
<span class="k">COPY</span><span class="s"> . /app</span>
</code></pre></div></div> <p>In this example, the entire context directory, represented by . (dot), is copied into the image. This approach can inadvertently include unwanted files that may not be necessary for the application. It can bloat the image size and potentially expose sensitive files or credentials to the container.</p> <p>Yes:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Create app directory</span>
<span class="k">RUN </span><span class="nb">mkdir</span> /app

<span class="c"># Copy only necessary files</span>
<span class="k">COPY</span><span class="s"> app.py requirements.txt /app/</span>
</code></pre></div></div> <p>In this improved example, specific files (app.py and requirements.txt) are copied into a designated directory (/app). By explicitly specifying the required files, you ensure that only the necessary files are included in the image. This approach helps keep the image size minimal and avoids exposing any unwanted or sensitive files to the container.</p> <h3 id="27-document-your-dockerfile">2.7 Document your Dockerfile</h3> <p><a id="s2.7-document-your-dockerfile"></a></p> <p>Include comments in your Dockerfile to provide context and explanations for the various instructions. This helps other developers understand the purpose and functionality of the Dockerfile.</p> <p>No:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Install dependencies</span>
<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> python3

<span class="c"># Set working directory</span>
<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="c"># Copy application files</span>
<span class="k">COPY</span><span class="s"> . /app</span>

<span class="c"># Run the application</span>
<span class="k">CMD</span><span class="s"> ["python3", "app.py"]</span>
</code></pre></div></div> <p>In this example, there is no explicit documentation or comments to explain the purpose or functionality of each instruction in the Dockerfile. It can make it challenging for other developers or maintainers to understand the intended usage or any specific requirements.</p> <p>Yes:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Install Python</span>
<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> python3

<span class="c"># Set working directory</span>
<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="c"># Copy application files</span>
<span class="k">COPY</span><span class="s"> . /app</span>

<span class="c"># Set the entrypoint command to run the application</span>
<span class="k">CMD</span><span class="s"> ["python3", "app.py"]</span>

<span class="c"># </span>
port 8000 for accessing the application
<span class="k">EXPOSE</span><span class="s"> 8000</span>

<span class="c"># Document the purpose of the image and any additional details</span>
<span class="k">LABEL</span><span class="s"> maintainer="John Doe &lt;john@example.com&gt;"</span>
<span class="k">LABEL</span><span class="s"> description="Docker image for running the example application."</span>
<span class="k">LABEL</span><span class="s"> version="1.0"</span>
</code></pre></div></div> <p>In this improved example, the Dockerfile is better documented:</p> <ul> <li>Each instruction is accompanied by a comment or description that explains its purpose.</li> <li>The <code class="language-plaintext highlighter-rouge">LABEL</code> instructions are used to provide additional information about the image, such as the maintainer, description, and version.</li> <li>The <code class="language-plaintext highlighter-rouge">EXPOSE</code> instruction documents the port that should be exposed for accessing the application.</li> </ul> <h3 id="28-usedockerignore-file">2.8 Use .dockerignore file</h3> <p><a id="s2.8-use-dockerignore-file"></a></p> <p>The .dockerignore file allow you to exclude files the context like a .gitignore file allow you to exclude files from your git repository. It helps to make build faster and lighter by excluding from the context big files or repository that are not used in the build.</p> <p>No:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Copy all files into the image</span>
<span class="k">COPY</span><span class="s"> . /app</span>

<span class="c"># Build the application</span>
<span class="k">RUN </span>make build
</code></pre></div></div> <p>In this example, all files in the current directory are copied into the image, including unnecessary files such as development tools, build artifacts, or sensitive information. This can bloat the image size and potentially expose unwanted or sensitive files to the container.</p> <p>Yes:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Copy only necessary files into the image</span>
<span class="k">COPY</span><span class="s"> . /app</span>

<span class="c"># Build the application</span>
<span class="k">RUN </span>make build
</code></pre></div></div> <p>.dockerignore:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>.git
node_modules
*.log
*.tmp
</code></pre></div></div> <p>In this improved example, a <code class="language-plaintext highlighter-rouge">.dockerignore</code> file is used to exclude unnecessary files and directories from being copied into the image. The .dockerignore file specifies patterns of files and directories that should be ignored during the build process. It helps reduce the image size, improve build performance, and avoid including unwanted files.</p> <p>The <code class="language-plaintext highlighter-rouge">.dockerignore</code> file in this example excludes the <code class="language-plaintext highlighter-rouge">.git</code> directory, the <code class="language-plaintext highlighter-rouge">node_modules</code> directory (common for Node.js projects), and files with extensions <code class="language-plaintext highlighter-rouge">.log</code> and <code class="language-plaintext highlighter-rouge">.tmp</code>. These files and directories are typically not needed in the final image and can be safely ignored.</p> <h3 id="29-test-your-image">2.9 Test your image</h3> <p><a id="s2.9-test-your-image"></a></p> <p>After building your Docker image, run it in a container to verify that everything works as expected. This ensures that your image is functional and can be used with confidence.</p> <p>No:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Copy application files</span>
<span class="k">COPY</span><span class="s"> . /app</span>

<span class="c"># Install dependencies</span>
<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> python3

<span class="c"># Run the application</span>
<span class="k">CMD</span><span class="s"> ["python3", "app.py"]</span>
</code></pre></div></div> <p>In this example, there is no explicit provision for testing the image. The Dockerfile only focuses on setting up the application, without any dedicated steps or considerations for running tests.</p> <p>Yes:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Copy application files</span>
<span class="k">COPY</span><span class="s"> . /app</span>

<span class="c"># Install dependencies</span>
<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> python3

<span class="c"># Run tests</span>
<span class="k">RUN </span>python3 <span class="nt">-m</span> unittest discover tests

<span class="c"># Run the application</span>
<span class="k">CMD</span><span class="s"> ["python3", "app.py"]</span>
</code></pre></div></div> <p>In this improved example, a dedicated step is added to run tests within the Dockerfile. The <code class="language-plaintext highlighter-rouge">RUN</code> instruction executes the necessary command to run tests using a testing framework (in this case, <code class="language-plaintext highlighter-rouge">unittest</code> is used as an example). By including this step, you ensure that tests are executed during the Docker image build process.</p> <p>It’s important to note that this example assumes the tests are included in a <code class="language-plaintext highlighter-rouge">tests</code> directory within the project structure. Adjust the command (<code class="language-plaintext highlighter-rouge">python3 -m unittest discover tests</code>) as per your project’s testing setup.</p> <h3 id="210-add-or-copy">2.10 ADD or COPY</h3> <p><a id="s2.10-add-or-copy"></a></p> <p>Although <code class="language-plaintext highlighter-rouge">ADD</code> and <code class="language-plaintext highlighter-rouge">COPY</code> are functionally similar, generally speaking, <code class="language-plaintext highlighter-rouge">COPY</code> is preferred. That’s because it’s more transparent than <code class="language-plaintext highlighter-rouge">ADD</code>. <code class="language-plaintext highlighter-rouge">COPY</code> only supports the basic copying of local files into the container, while <code class="language-plaintext highlighter-rouge">ADD</code> has some features (like local-only tar extraction and remote URL support) that are not immediately obvious. Consequently, the best use for <code class="language-plaintext highlighter-rouge">ADD</code> is local tar file auto-extraction into the image, as in <code class="language-plaintext highlighter-rouge">ADD rootfs.tar.xz /.</code></p> <p>If you have multiple Dockerfile steps that use different files from your context, <code class="language-plaintext highlighter-rouge">COPY</code> them individually, rather than all at once. This ensures that each step’s build cache is only invalidated, forcing the step to be re-run if the specifically required files change.</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">COPY</span><span class="s"> requirements.txt /tmp/</span>
<span class="k">RUN </span>pip <span class="nb">install</span> <span class="nt">--requirement</span> /tmp/requirements.txt
<span class="k">COPY</span><span class="s"> . /tmp/</span>
</code></pre></div></div> <p>Results in fewer cache invalidations for the <code class="language-plaintext highlighter-rouge">RUN</code> step, than if you put the <code class="language-plaintext highlighter-rouge">COPY . /tmp/</code> before it.</p> <p>Because image size matters, using <code class="language-plaintext highlighter-rouge">ADD</code> to fetch packages from remote URLs is strongly discouraged; you should use curl or wget instead. That way you can delete the files you no longer need after they’ve been extracted and you don’t have to add another layer in your image. For example, you should avoid doing things like:</p> <p>No:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">ADD</span><span class="s"> https://example.com/big.tar.xz /usr/src/things/</span>
<span class="k">RUN </span><span class="nb">tar</span> <span class="nt">-xJf</span> /usr/src/things/big.tar.xz <span class="nt">-C</span> /usr/src/things
<span class="k">RUN </span>make <span class="nt">-C</span> /usr/src/things all
</code></pre></div></div> <p>And instead, do something like:</p> <p>Yes:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code> RUN mkdir -p /usr/src/things \
  &amp;&amp; curl -SL https://example.com/big.tar.xz \
  | tar -xJC /usr/src/things \
  &amp;&amp; make -C /usr/src/things all
</code></pre></div></div> <p>For other items, like files and directories, that don’t require the tar auto-extraction capability of <code class="language-plaintext highlighter-rouge">ADD</code>, you should always use <code class="language-plaintext highlighter-rouge">COPY</code>.</p> <p>For more information about ADD or COPY, see the following:</p> <ul> <li><a href="https://docs.docker.com/engine/reference/builder/#add">Dockerfile reference for the ADD instruction</a></li> <li><a href="https://docs.docker.com/engine/reference/builder/#copy">Dockerfile reference for the COPY instruction</a></li> </ul> <p>Reference: <a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#:~:text=COPY%20only%20supports%20the%20basic,rootfs.tar.xz%20%2F%20.">https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#:~:text=COPY%20only%20supports%20the%20basic,rootfs.tar.xz%20%2F%20</a></p> <h2 id="3-security-practices">3. Security practices</h2> <p><a id="s3-security-practices"></a></p> <h3 id="31-use-environment-variables-for-configuration">3.1 Use environment variables for configuration</h3> <p><a id="s3.1-use-environment-variables-for-configuration"></a></p> <p>Instead of hardcoding configuration values inside the Dockerfile, use environment variables. This allows for greater flexibility and easier configuration management. You can set these variables when running the container.</p> <p>No:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Set configuration values directly</span>
<span class="k">ENV</span><span class="s"> DB_HOST=localhost</span>
<span class="k">ENV</span><span class="s"> DB_PORT=3306</span>
<span class="k">ENV</span><span class="s"> DB_USER=myuser</span>
<span class="k">ENV</span><span class="s"> DB_PASSWORD=mypassword</span>

<span class="c"># Run the application</span>
<span class="k">CMD</span><span class="s"> ["python3", "app.py"]</span>
</code></pre></div></div> <p>In this example, configuration values are directly set as environment variables using the ENV instruction in the Dockerfile. This approach has a few drawbacks:</p> <ul> <li>Configuration values are hardcoded in the Dockerfile, making it less flexible and harder to change without modifying the file itself.</li> <li>Sensitive information, such as passwords or API keys, is exposed in plain text in the Dockerfile, which is not secure.</li> </ul> <p>Yes:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Set default configuration values</span>
<span class="k">ENV</span><span class="s"> DB_HOST=localhost</span>
<span class="k">ENV</span><span class="s"> DB_PORT=3306</span>
<span class="k">ENV</span><span class="s"> DB_USER=defaultuser</span>
<span class="k">ENV</span><span class="s"> DB_PASSWORD=defaultpassword</span>

<span class="c"># Run the application</span>
<span class="k">CMD</span><span class="s"> ["python3", "app.py"]</span>
</code></pre></div></div> <p>In this improved example, default configuration values are set as environment variables, but they are kept generic and non-sensitive. This approach provides a template for configuration that can be customized when running the container.</p> <p>To securely provide sensitive configuration values, you can pass them as environment variables during runtime using the -e flag with the docker run command or by using a secrets management solution like Docker Secrets or environment-specific .env files.</p> <p>For example, when running the container, you can override the default values:</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">docker run -e DB_HOST=mydbhost -e DB_PORT=5432 -e DB_USER=myuser -e DB_PASSWORD=mypassword myimage
</span></code></pre></div></div> <h4 id="311-setting-dynamic-environment-values-arg-vs-env">3.1.1 Setting Dynamic Environment Values (ARG vs ENV)</h4> <p><a id="s3.1.1-setting-dynamic-environment-values-arg-vs-env"></a></p> <p>Dockerfile doesn’t provide a dynamic tool to set an ENV value during the build process. However, there’s a solution to this problem. We have to use ARG. ARG values don’t work in the same way as ENV, as we can’t access them anymore once the image is built.</p> <p><img src="https://github.com/chrislevn/dockerfile-practices/assets/32094007/b45d08b5-6bb4-44db-bf0c-2a8b1d8e7ed6" alt="image"/></p> <p>Let’s see how we can work around this issue:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">ARG</span><span class="s"> name</span>
<span class="k">ENV</span><span class="s"> env_name $name</span>
</code></pre></div></div> <p>We’ll introduce the name ARG variable. Then we’ll use it to assign a value to the env_name environment variable using ENV. When we want to set this argument, we’ll pass it with the –build-arg flag:</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">docker build -t image_name --build-arg name=Christopher .
</span></code></pre></div></div> <p>Now we’ll run our container. We should see:</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">Hello Christopher
</span></code></pre></div></div> <p>Reference: <a href="https://www.baeldung.com/ops/dockerfile-env-variable">https://www.baeldung.com/ops/dockerfile-env-variable</a></p> <h3 id="33-set-the-correct-container-user">3.3 Set the correct container user</h3> <p><a id="s3.3-set-the-correct-container-user"></a></p> <p>By default, Docker runs containers as the root user. To improve security, create a dedicated user for running your application within the container and switch to that user using the <code class="language-plaintext highlighter-rouge">USER</code> instruction.</p> <p>No:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Set working directory</span>
<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="c"># Copy application files</span>
<span class="k">COPY</span><span class="s"> . /app</span>

<span class="c"># Set container user as root</span>
<span class="k">USER</span><span class="s"> root</span>

<span class="c"># Run the application</span>
<span class="k">CMD</span><span class="s"> ["python3", "app.py"]</span>
</code></pre></div></div> <p>In this example, the container user is set to root using the USER instruction. Running the container as the root user can pose security risks, as any malicious code or vulnerability exploited within the container would have elevated privileges.</p> <p>Why it’s important:</p> <ul> <li> <p>Running processes within a container as a non-root user minimizes the potential damage that can be caused by security vulnerabilities</p> </li> <li> <p>Following the principle of least privilege, a non-root user only has access to the resources and permissions necessary to perform its intended tasks. This reduces the risk of accidental or intentional misuse of privileged operations within the container.</p> </li> <li> <p>Running containers with a non-root user adds an additional layer of isolation between the containerized application and the host system. This isolation helps protect the host system from unintended changes or malicious activities within the container.</p> </li> <li> <p>Many organizations and regulatory frameworks require the use of non-root users for security and compliance purposes. Adhering to these best practices can help meet these requirements and ensure that your containerized applications pass security audits.</p> </li> </ul> <p>Yes:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Set working directory</span>
<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="c"># Copy application files</span>
<span class="k">COPY</span><span class="s"> . /app</span>

<span class="c"># Create a non-root user</span>
<span class="k">RUN </span>groupadd <span class="nt">-r</span> myuser <span class="o">&amp;&amp;</span> useradd <span class="nt">-r</span> <span class="nt">-g</span> myuser myuser

<span class="c"># Set ownership and permissions</span>
<span class="k">RUN </span><span class="nb">chown</span> <span class="nt">-R</span> myuser:myuser /app

<span class="c"># Switch to the non-root user</span>
<span class="k">USER</span><span class="s"> myuser</span>

<span class="c"># Run the application</span>
<span class="k">CMD</span><span class="s"> ["python3", "app.py"]</span>
</code></pre></div></div> <p>In this improved example, a dedicated non-root user (myuser) is created using the useradd and groupadd commands. The ownership and permissions of the /app directory are changed to the non-root user using chown. Finally, the USER instruction switches to the non-root user before running the application.</p> <h3 id="34-create-a-non-root-user-in-the-dockerfile">3.4 Create a non-root user in the Dockerfile</h3> <p><a id="s3.4-create-a-non-root-user-in-the-dockerfile"></a></p> <p>Start your Dockerfile with a base image that already has a non-root user defined. This will ensure that your container starts with a non-root user by default</p> <p>If your base image doesn’t provide a non-root user, you should create one in your Dockerfile using the USER and RUN instructions. Specify a unique username and a non-privileged user ID (UID) for the user. This can be achieved with the following lines in your Dockerfile:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">RUN </span>&lt;instructions that require to run with root privileges&gt;
<span class="k">RUN </span>addgroup <span class="nt">--system</span> nonroot <span class="o">&amp;&amp;</span> adduser <span class="nt">--system</span> <span class="nt">--ingroup</span> nonroot nonroot
<span class="k">USER</span><span class="s"> nonroot</span>
</code></pre></div></div> <p>Ensure that the non-root user has the necessary permissions to execute the required commands and access the required files and directories within the container. Use the RUN instruction with <code class="language-plaintext highlighter-rouge">chown</code> or <code class="language-plaintext highlighter-rouge">chmod</code> to adjust the ownership and permissions as needed.</p> <h3 id="35-avoid-running-containers-with-root-privileges">3.5 Avoid running containers with root privileges</h3> <p><a id="s3.5-avoid-running-containers-with-root-privileges"></a></p> <p>When starting the container, avoid running it as the root user. Instead, specify the <code class="language-plaintext highlighter-rouge">non-root</code> user as the user to run the container using the <code class="language-plaintext highlighter-rouge">--user</code> flag with the docker run command or the equivalent in your container orchestration platform.</p> <p>Why it’s important:</p> <ul> <li> <p>Running processes within a container as a non-root user minimizes the potential damage that can be caused by security vulnerabilities</p> </li> <li> <p>Following the principle of least privilege, a non-root user only has access to the resources and permissions necessary to perform its intended tasks. This reduces the risk of accidental or intentional misuse of privileged operations within the container.</p> </li> <li> <p>Running containers with a non-root user adds an additional layer of isolation between the containerized application and the host system. This isolation helps protect the host system from unintended changes or malicious activities within the container.</p> </li> <li> <p>Many organizations and regulatory frameworks require the use of non-root users for security and compliance purposes. Adhering to these best practices can help meet these requirements and ensure that your containerized applications pass security audits.</p> </li> </ul> <h2 id="4-other-references">4. Other references:</h2> <p><a id="s4-other-references"></a></p> <h3 id="41-expose">4.1 EXPOSE</h3> <p><a id="s4.1-expose"></a></p> <p>The <code class="language-plaintext highlighter-rouge">EXPOSE</code> instruction informs Docker that the container listens on the specified network ports at runtime. EXPOSE does not make the ports of the container accessible to the host.</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> nginx:latest</span>

<span class="c"># Expose port 80 for HTTP traffic</span>
<span class="k">EXPOSE</span><span class="s"> 80</span>
</code></pre></div></div> <p>In this example, the <code class="language-plaintext highlighter-rouge">EXPOSE</code> instruction is used to document that the containerized Nginx web server is expected to listen on port 80 for HTTP traffic. Users who want to connect to the running container can refer to the EXPOSE instruction to determine which ports should be accessed.</p> <p>To make the exposed ports accessible from the host machine, you need to publish them when running the container using the -p or -P option of the docker run command.</p> <p>For example, to publish port 80 of a container to port 8080 on the host machine:</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">docker run -p 8080:80 myimage
</span></code></pre></div></div> <h3 id="42-entrypoint-vs-cmd-vs-run">4.2 ENTRYPOINT vs CMD vs RUN</h3> <p><a id="s4.2-entrypoint-vs-cmd-vs-run"></a></p> <ul> <li><code class="language-plaintext highlighter-rouge">ENTRYPOINT</code>: The <code class="language-plaintext highlighter-rouge">ENTRYPOINT</code> instruction specifies the primary command to be executed when a container is run from an image. It sets the entrypoint for the container, which means it provides the default executable for the container. It is typically used to specify the main command or process that the container should run. You can use <code class="language-plaintext highlighter-rouge">ENTRYPOINT</code> in either the shell form (as a command string) or the exec form (as an array of strings).</li> </ul> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Set the entrypoint command as an array</span>
<span class="k">ENTRYPOINT</span><span class="s"> ["echo", "Hello, World!"]</span>
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">CMD</code>: The <code class="language-plaintext highlighter-rouge">CMD</code> instruction provides default arguments for the entrypoint command defined by <code class="language-plaintext highlighter-rouge">ENTRYPOINT</code>. It sets the default parameters or arguments that will be passed to the entrypoint command when the container starts. <code class="language-plaintext highlighter-rouge">CMD</code> can also be specified in either the shell form (as a command string) or the exec form (as an array of strings). If the <code class="language-plaintext highlighter-rouge">CMD</code> instruction is present in the Dockerfile, it will be overridden by any command line arguments passed to the docker run command when starting the container.</li> </ul> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Set the entrypoint command as an array</span>
<span class="k">ENTRYPOINT</span><span class="s"> ["echo"]</span>

<span class="c"># Set the default argument for the entrypoint command</span>
<span class="k">CMD</span><span class="s"> ["Hello, World!"]</span>
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">RUN</code>: The <code class="language-plaintext highlighter-rouge">RUN</code> instruction is used to execute commands during the build process of the Docker image. It runs commands within the image’s file system and creates a new layer with the changes made by the commands. <code class="language-plaintext highlighter-rouge">RUN</code> is typically used for installing dependencies, configuring the environment, or performing any actions needed to set up the image for runtime. Each <code class="language-plaintext highlighter-rouge">RUN</code> instruction creates a new layer in the Docker image, and the changes made by the command are preserved in that layer.</li> </ul> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>

<span class="c"># Run a command during the build process</span>
<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> curl
</code></pre></div></div> <p>More on <code class="language-plaintext highlighter-rouge">ENTRYPOINT</code>: In Docker, the <code class="language-plaintext highlighter-rouge">ENTRYPOINT</code> instruction is used in a Dockerfile to specify the primary command that should be run when a container is started from the image. It sets the executable that will be invoked by default when the container is run as an executable.</p> <p>Shell form:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>
<span class="k">ENTRYPOINT</span><span class="s"> echo "Hello, World!"</span>
</code></pre></div></div> <p>Exec form:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:20.04</span>
<span class="k">ENTRYPOINT</span><span class="s"> ["/bin/echo", "Hello, World!"]</span>
</code></pre></div></div> <p>In the shell form, the <code class="language-plaintext highlighter-rouge">ENTRYPOINT</code> instruction is specified as a command string. This command string is interpreted as a shell command, allowing for shell processing, variable substitution, and other shell features.</p> <p>In the exec form, the <code class="language-plaintext highlighter-rouge">ENTRYPOINT</code> instruction is specified as an array of strings. The first element of the array is the executable, and subsequent elements are passed as arguments to the executable.</p> <p>The ENTRYPOINT instruction provides a way to set a default command or executable for the container. Any additional parameters passed when running the container will be appended to the <code class="language-plaintext highlighter-rouge">ENTRYPOINT</code> command, allowing for flexibility and parameterization.</p> <p>In Python, here is how <code class="language-plaintext highlighter-rouge">ENTRYPOINT</code> can be used with <code class="language-plaintext highlighter-rouge">CMD</code>:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> python:3.9-slim-buster</span>

<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="c"># Copy the Python application code</span>
<span class="k">COPY</span><span class="s"> app.py .</span>

<span class="c"># Set the entrypoint command</span>
<span class="k">ENTRYPOINT</span><span class="s"> ["python3"]</span>

<span class="c"># Set the default arguments for the entrypoint command</span>
<span class="k">CMD</span><span class="s"> ["app.py"]</span>
</code></pre></div></div> <p>In this example, the Dockerfile starts with a Python base image (<code class="language-plaintext highlighter-rouge">python:3.9-slim-buster</code>) and sets the working directory to <code class="language-plaintext highlighter-rouge">/app</code>. The Python application code file (<code class="language-plaintext highlighter-rouge">app.py</code>) is then copied into the image.</p> <p>The <code class="language-plaintext highlighter-rouge">ENTRYPOINT</code> instruction specifies the command that will be executed when the container starts. In this case, it sets the command to python3, which is the Python interpreter.</p> <p>The <code class="language-plaintext highlighter-rouge">CMD</code> instruction provides default arguments to the <code class="language-plaintext highlighter-rouge">ENTRYPOINT</code> command. In this case, the default argument is app.py, representing the Python script that will be executed.</p> <p>When you build and run the container, the Python application specified by app.py will be executed as the primary command. However, if you provide additional arguments when running the container, they will override the default arguments specified by <code class="language-plaintext highlighter-rouge">CMD</code>.</p> <h3 id="43-docker-image-vs-docker-containers">4.3 Docker Image vs Docker Containers:</h3> <p><a id="s4.3-docker-image-vs-docker-containers"></a></p> <p><img src="https://github.com/chrislevn/dockerfile-practices/assets/32094007/05fa9dc6-82b6-4fd9-be27-759d46632e25" alt="image"/></p> <h4 id="431-docker-image">4.3.1 Docker Image:</h4> <p><a id="s4.3.1-docker-image"></a></p> <p>A Docker image is a lightweight, standalone, and executable package that contains everything needed to run a piece of software, including the code, runtime environment, libraries, dependencies, and system tools. It is created from a Dockerfile, which specifies the instructions for building the image. Images are immutable, meaning they are read-only and cannot be modified once created. You can think of an image as a blueprint or template for creating containers.</p> <h4 id="432-docker-container">4.3.2 Docker Container:</h4> <p><a id="s4.3.2-docker-container"></a></p> <p>A Docker container is a running instance of an image. It is a lightweight and isolated runtime environment that encapsulates an application and its dependencies. Containers are created from Docker images and can be started, stopped, paused, restarted, and deleted as needed. Each container runs in isolation, utilizing the host system’s resources efficiently while providing a consistent environment for the application to run. Containers are transient and can be recreated easily from the corresponding image.</p> <h4 id="433-docker-image-vs-containers">4.3.3 Docker Image vs Containers</h4> <p><a id="s4.3.3-docker-image-vs-containers"></a></p> <ul> <li>The key difference between a Docker image Vs a container is that a Docker image is a read-only immutable template that defines how a container will be realized. A Docker container is a runtime instance of a Docker image that gets created when the $ docker run command is implemented.</li> <li>Before the docker container can even exist docker templates/images are built using $ docker build CLI.</li> <li>Docker image templates can exist in isolation but containers can’t exist without images.</li> <li>So docker image is an integral part of containers that differs only because of their objectives which we have already covered.</li> <li>Docker images can’t be paused or started but a Docker container is a run time instance that can be started or paused.</li> </ul> <p>Reference: https://www.knowledgehut.com/blog/devops/docker-vs-container</p> <h3 id="44-workdir">4.4 WORKDIR</h3> <p><a id="s4.4-workdir"></a></p> <p>In a Dockerfile, the <code class="language-plaintext highlighter-rouge">WORKDIR</code> instruction is used to set the working directory for any subsequent instructions in the Dockerfile. It is similar to the cd command in Linux or Unix systems.</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">WORKDIR</span><span class="s"> /path/to/directory</span>
</code></pre></div></div> <p>Here, <code class="language-plaintext highlighter-rouge">/path/to/directory</code> is the absolute or relative path to the directory you want to set as the working directory. If the directory does not exist, Docker will create it.</p> <p>The <code class="language-plaintext highlighter-rouge">WORKDIR</code> instruction affects subsequent instructions like <code class="language-plaintext highlighter-rouge">RUN</code>, <code class="language-plaintext highlighter-rouge">CMD</code>, <code class="language-plaintext highlighter-rouge">COPY</code>, and <code class="language-plaintext highlighter-rouge">ADD</code>. Any relative paths specified in these instructions will be resolved relative to the working directory set by <code class="language-plaintext highlighter-rouge">WORKDIR</code>.</p> <p>It’s recommended to use absolute paths for better clarity and predictability in your Dockerfile.</p> <p>Here’s an example of how <code class="language-plaintext highlighter-rouge">WORKDIR</code> can be used in a Dockerfile:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:latest</span>

<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="k">COPY</span><span class="s"> . /app</span>

<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> <span class="se">\
</span>    apt-get <span class="nb">install</span> <span class="nt">-y</span> python3

<span class="k">CMD</span><span class="s"> ["python3", "app.py"]</span>
</code></pre></div></div> <p>For clarity and reliability, you should always use absolute paths for your WORKDIR. Also, you should use <code class="language-plaintext highlighter-rouge">WORKDIR</code> instead of proliferating instructions like <code class="language-plaintext highlighter-rouge">RUN cd … &amp;&amp; do-something</code>, which are hard to read, troubleshoot, and maintain.</p> <p>For more information about USER, see <a href="https://docs.docker.com/engine/reference/builder/#user">Dockerfile reference for the USER instruction</a>.</p> <h3 id="45-volume">4.5 VOLUME</h3> <p><a id="s4.5-volume"></a></p> <p>In a Dockerfile, the <code class="language-plaintext highlighter-rouge">VOLUME</code> instruction is used to create a mount point and designate a directory as a volume for persistent data storage or sharing between containers and the host system.</p> <p>The syntax for the <code class="language-plaintext highlighter-rouge">VOLUME</code> instruction is as follows:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">VOLUME</span><span class="s"> ["/path/to/volume"]</span>
</code></pre></div></div> <p>Here, “/path/to/volume” specifies the absolute path to the directory that you want to designate as a volume.</p> <p>When you run a container using an image that includes a <code class="language-plaintext highlighter-rouge">VOLUME</code> instruction, Docker creates a mount point at the specified path and sets it as a volume. Any data written to that directory inside the container will be stored in the volume. The data in the volume persists even after the container is stopped or removed.</p> <p>Volumes are typically used for storing databases, logs, configuration files, or any other data that needs to persist beyond the lifecycle of a container. They provide a way to separate the storage of data from the container itself, making it easier to manage and migrate containers without losing important data.</p> <p>You can specify multiple VOLUME instructions in a Dockerfile to create multiple volumes.</p> <p>Here’s an example of how the VOLUME instruction can be used in a Dockerfile:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:latest</span>

<span class="k">VOLUME</span><span class="s"> ["/app/data", "/app/logs"]</span>

<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="k">COPY</span><span class="s"> . /app</span>

<span class="k">CMD</span><span class="s"> ["python3", "app.py"]</span>
</code></pre></div></div> <p>In this example, the <code class="language-plaintext highlighter-rouge">VOLUME</code> instruction creates two volumes: <code class="language-plaintext highlighter-rouge">/app/data</code> and <code class="language-plaintext highlighter-rouge">/app/logs</code>. Any data written to these directories inside the container will be stored in the respective volumes. These volumes can then be accessed or managed using Docker commands or through container orchestration tools.</p> <p>The <code class="language-plaintext highlighter-rouge">VOLUME</code> instruction should be used to expose any database storage area, configuration storage, or files and folders created by your Docker container. You are strongly encouraged to use VOLUME for any combination of mutable or user-serviceable parts of your image.</p> <p>For more information about <code class="language-plaintext highlighter-rouge">VOLUME</code>, see <a href="https://docs.docker.com/engine/reference/builder/#volume">Dockerfile reference for the <code class="language-plaintext highlighter-rouge">VOLUME</code> instruction</a>.</p> <h3 id="46-user">4.6 USER</h3> <p><a id="s4.6-user"></a></p> <p>In a Dockerfile, the USER instruction is used to specify the user or UID (user identifier) that the container should run as when executing subsequent instructions.</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">USER</span><span class="s"> user[:group]</span>
</code></pre></div></div> <p>Here, user can be either the username or the UID of the user you want to set as the user for the container. Optionally, you can specify group to set the group for the user as well.</p> <p>The <code class="language-plaintext highlighter-rouge">USER</code> instruction is often used to run the container with a non-root user for security reasons. By default, Docker containers run as the root user (UID 0), which can pose security risks. Running the container as a non-root user helps to minimize the impact of potential security vulnerabilities.</p> <p>You can specify the user by either using the username or the UID. If you provide a username, Docker will try to resolve it to the corresponding UID and GID (group identifier) within the container. If you provide a UID directly, Docker will use that UID and assign it to the user.</p> <p>Here’s an example of how the <code class="language-plaintext highlighter-rouge">USER</code> instruction can be used in a Dockerfile:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:latest</span>

<span class="k">RUN </span>groupadd <span class="nt">-r</span> mygroup <span class="o">&amp;&amp;</span> useradd <span class="nt">-r</span> <span class="nt">-g</span> mygroup myuser

<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="k">COPY</span><span class="s"> . /app</span>

<span class="k">USER</span><span class="s"> myuser</span>

<span class="k">CMD</span><span class="s"> ["python3", "app.py"]</span>
</code></pre></div></div> <p>In this example, the Dockerfile creates a new user named myuser and a group named mygroup. The <code class="language-plaintext highlighter-rouge">USER</code> instruction sets myuser as the user for subsequent instructions, starting from the <code class="language-plaintext highlighter-rouge">CMD</code> instruction. This ensures that the container runs with the specified user rather than the root user.</p> <p>If a service can run without privileges, use <code class="language-plaintext highlighter-rouge">USER</code> to change to a non-root user. Start by creating the user and group in the Dockerfile with something like the following example:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">RUN </span>groupadd <span class="nt">-r</span> postgres <span class="o">&amp;&amp;</span> useradd <span class="nt">--no-log-init</span> <span class="nt">-r</span> <span class="nt">-g</span> postgres postgres
</code></pre></div></div> <h4 id="note">Note:</h4> <p>Consider an explicit UID/GID.</p> <p>Users and groups in an image are assigned a non-deterministic UID/GID in that the “next” UID/GID is assigned regardless of image rebuilds. So, if it’s critical, you should assign an explicit UID/GID.</p> <p>Avoid installing or using sudo as it has unpredictable <code class="language-plaintext highlighter-rouge">TTY</code> and signal-forwarding behavior that can cause problems. If you absolutely need functionality similar to <code class="language-plaintext highlighter-rouge">sudo</code>, such as initializing the daemon as <code class="language-plaintext highlighter-rouge">root</code> but running it as non-root, consider using “gosu”.</p> <p>Lastly, to reduce layers and complexity, avoid switching <code class="language-plaintext highlighter-rouge">USER</code> back and forth frequently.</p> <p>For more information about <code class="language-plaintext highlighter-rouge">USER</code>, see <a href="https://docs.docker.com/engine/reference/builder/#user">Dockerfile reference for the <code class="language-plaintext highlighter-rouge">USER</code> instruction</a>.</p> <p><a id="s3.6-user"></a></p> <h3 id="47-onbuild">4.7 ONBUILD</h3> <p><a id="s4.7-onbuild"></a></p> <p>The <code class="language-plaintext highlighter-rouge">ONBUILD</code> instruction is used to add triggers to an image that will be executed when the image is used as the base for another Docker image. It allows you to define actions that should be performed in child images without modifying the parent image.</p> <p>The syntax for the <code class="language-plaintext highlighter-rouge">ONBUILD</code> instruction is as follows:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ONBUILD &lt;INSTRUCTION&gt;
</code></pre></div></div> <p>Here, <code class="language-plaintext highlighter-rouge">&lt;INSTRUCTION&gt;</code> can be any valid Dockerfile instruction like <code class="language-plaintext highlighter-rouge">RUN</code>, <code class="language-plaintext highlighter-rouge">COPY</code>, <code class="language-plaintext highlighter-rouge">CMD</code>, etc. It represents the action or instruction that should be executed in the child image.</p> <p>When an image with an <code class="language-plaintext highlighter-rouge">ONBUILD</code> instruction is used as the base image for another Docker image, the specified instruction is recorded and saved in the metadata of the parent image. Then, when the child image is built, Docker triggers and executes those recorded instructions as part of the child image build process.</p> <p>The <code class="language-plaintext highlighter-rouge">ONBUILD</code> instruction is typically used to automate certain tasks or actions that are common to many derived images. For example, you can use it to specify actions like copying files into the image, setting environment variables, or running commands.</p> <p>Here’s an example to illustrate the usage of <code class="language-plaintext highlighter-rouge">ONBUILD</code> in a Dockerfile:</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:latest</span>

<span class="k">ONBUILD</span><span class="w"> </span><span class="k">COPY</span><span class="s"> . /app</span>
<span class="k">ONBUILD RUN </span>make /app

<span class="k">CMD</span><span class="s"> ["./app"]</span>
</code></pre></div></div> <p>In this example, the parent image specifies two <code class="language-plaintext highlighter-rouge">ONBUILD</code> instructions. The first <code class="language-plaintext highlighter-rouge">ONBUILD</code> instruction records the <code class="language-plaintext highlighter-rouge">COPY . /app</code> instruction, which copies files from the context directory into the <code class="language-plaintext highlighter-rouge">/app</code> directory of the child image. The second <code class="language-plaintext highlighter-rouge">ONBUILD</code> instruction records the <code class="language-plaintext highlighter-rouge">RUN</code> make <code class="language-plaintext highlighter-rouge">/app</code> instruction, which builds the application in the child image.</p> <p>When the child image is built using this parent image, Docker will automatically execute the recorded <code class="language-plaintext highlighter-rouge">COPY . /app</code> and <code class="language-plaintext highlighter-rouge">RUN make /app</code> instructions in the child image build context. Finally, the <code class="language-plaintext highlighter-rouge">CMD</code> instruction will run the built application in the child image.</p> <p>An <code class="language-plaintext highlighter-rouge">ONBUILD</code> command executes after the current Dockerfile build completes. <code class="language-plaintext highlighter-rouge">ONBUILD</code> executes in any child image derived <code class="language-plaintext highlighter-rouge">FROM</code> the current image. Think of the <code class="language-plaintext highlighter-rouge">ONBUILD</code> command as an instruction that the parent Dockerfile gives to the child Dockerfile.</p> <p>A Docker build executes <code class="language-plaintext highlighter-rouge">ONBUILD</code> commands before any command in a child Dockerfile.</p> <p><code class="language-plaintext highlighter-rouge">ONBUILD</code> is useful for images that are going to be built <code class="language-plaintext highlighter-rouge">FROM</code> a given image. For example, you would use <code class="language-plaintext highlighter-rouge">ONBUILD</code> for a language stack image that builds arbitrary user software written in that language within the Dockerfile, as you can see in Ruby’s <code class="language-plaintext highlighter-rouge">ONBUILD</code> variants.</p> <p>Images built with <code class="language-plaintext highlighter-rouge">ONBUILD</code> should get a separate tag. For example, <code class="language-plaintext highlighter-rouge">ruby:1.9-onbuild</code> or <code class="language-plaintext highlighter-rouge">ruby:2.0-onbuild</code>.</p> <p>Be careful when putting <code class="language-plaintext highlighter-rouge">ADD</code> or <code class="language-plaintext highlighter-rouge">COPY</code> in <code class="language-plaintext highlighter-rouge">ONBUILD</code>. The onbuild image fails catastrophically if the new build’s context is missing the resource being added. Adding a separate tag, as recommended above, helps mitigate this by allowing the Dockerfile author to make a choice.</p> <p>For more information about ONBUILD, see <a href="https://docs.docker.com/engine/reference/builder/#onbuild">Dockerfile reference for the <code class="language-plaintext highlighter-rouge">ONBUILD</code> instruction</a>.</p> <p>Reference: <a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/">https://docs.docker.com/develop/develop-images/dockerfile_best-practices/</a></p> <h2 id="5-good-demo">5. Good demo</h2> <p><a id="s5-good-demo"></a></p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Use a suitable base image</span>
<span class="k">FROM</span><span class="s"> python:3.9-slim-buster</span>

<span class="c"># Set the working directory</span>
<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="c"># Copy only necessary files</span>
<span class="k">COPY</span><span class="s"> requirements.txt .</span>
<span class="k">COPY</span><span class="s"> app.py .</span>

<span class="c"># Install dependencies</span>
<span class="k">RUN </span>pip <span class="nb">install</span> <span class="nt">--no-cache-dir</span> <span class="nt">-r</span> requirements.txt

<span class="c"># Set the container user</span>
<span class="k">RUN </span>groupadd <span class="nt">-r</span> myuser <span class="o">&amp;&amp;</span> useradd <span class="nt">-r</span> <span class="nt">-g</span> myuser myuser
<span class="k">USER</span><span class="s"> myuser</span>

<span class="c"># Expose the necessary port</span>
<span class="k">EXPOSE</span><span class="s"> 8000</span>

<span class="c"># Set environment variables</span>
<span class="k">ENV</span><span class="s"> DB_HOST=localhost</span>
<span class="k">ENV</span><span class="s"> DB_PORT=3306</span>
<span class="k">ENV</span><span class="s"> DB_USER=defaultuser</span>
<span class="k">ENV</span><span class="s"> DB_PASSWORD=defaultpassword</span>

<span class="c"># Set the entrypoint command</span>
<span class="k">CMD</span><span class="s"> ["python3", "app.py"]</span>
</code></pre></div></div> <p>In this example, we follow several best practices:</p> <ul> <li>We use an appropriate base image (<code class="language-plaintext highlighter-rouge">python:3.9-slim-buster</code>) that provides a minimal Python environment.</li> <li>We set the working directory to <code class="language-plaintext highlighter-rouge">/app</code> to execute commands and copy files within that directory.</li> <li>Only necessary files (<code class="language-plaintext highlighter-rouge">requirements.txt</code> and <code class="language-plaintext highlighter-rouge">app.py</code>) are copied into the image, reducing unnecessary content. -Dependencies are installed using pip with the <code class="language-plaintext highlighter-rouge">--no-cache-dir</code> flag to avoid caching unnecessary artifacts. -A non-root user (<code class="language-plaintext highlighter-rouge">myuser</code>) is created and used to run the container, enhancing security. -The necessary port (<code class="language-plaintext highlighter-rouge">8000</code>) is exposed to allow access to the application. -Environment variables are set for configuring the database connection. -The <code class="language-plaintext highlighter-rouge">CMD</code> instruction specifies the command to run when the container starts.</li> </ul> <p>More demos on <a href="https://github.com/chrislevn/dockerfile-practices/tree/main/demo">https://github.com/chrislevn/dockerfile-practices/tree/main/demo</a></p> <h2 id="6-basic-steps-to-running-docker-file-with-docker-cli">6. Basic steps to running Docker file with docker cli:</h2> <p><a id="s6-basic-steps-to-running-docker-file-with-docker-cli"></a></p> <ul> <li>Make sure you have Docker installed and running on your system. You can check this by running the docker version command in your terminal or command prompt. If you have Docker Destop, make sure it is running.</li> <li>Create a Dockerfile in your project directory. The Dockerfile contains instructions for building your Docker image.</li> <li>Open a terminal or command prompt and navigate to the directory where your Dockerfile is located.</li> <li>Build the Docker image using the <code class="language-plaintext highlighter-rouge">docker build</code> command. Provide a tag for your image using the <code class="language-plaintext highlighter-rouge">-t</code> option. For example:</li> </ul> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">docker build -t myapp:1.0 .
</span></code></pre></div></div> <p>This command builds an image with the tag <code class="language-plaintext highlighter-rouge">myapp:1.0</code> using the Dockerfile in the current directory (<code class="language-plaintext highlighter-rouge">.</code>).</p> <p>Once the image is built, you can run a container based on that image using the docker run command. Specify the image name or tag with the -it option for an interactive session. For example:</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">docker run -it myapp:1.0
</span></code></pre></div></div> <p>This command starts a container based on the myapp:1.0 image. <code class="language-plaintext highlighter-rouge">-it</code> is short for <code class="language-plaintext highlighter-rouge">--interactive</code> + <code class="language-plaintext highlighter-rouge">--tty</code>. When you docker run with this command it takes you straight inside the container.</p> <p>Note: If your application requires ports to be exposed, you can use the -p option to map container ports to host ports. For example, to expose port <code class="language-plaintext highlighter-rouge">8000</code>:</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">docker run -it -p 8000:8000 myapp:1.0
</span></code></pre></div></div> <h3 id="61-running-docker-with-docker-compose">6.1 Running Docker with Docker compose</h3> <p><a id="s6.1-run-docker-with-docker-compose"></a></p> <h4 id="611-docker-compose">6.1.1 Docker compose</h4> <p><a id="s6.1.1-docker-compose"></a></p> <p>Docker Compose is a tool that allows you to define and manage multi-container Docker applications. It provides a way to describe the configuration of multiple services, networks, and volumes using a YAML file format. With Docker Compose, you can define a set of containers that make up your application, specify their configurations, and manage their lifecycle as a single unit.</p> <h4 id="612-docker-compose-vs-docker-run">6.1.2 Docker compose vs Docker run</h4> <p><a id="s6.1.2-docker-compose-vs-docker-run"></a></p> <ol> <li><strong>Docker Compose:</strong> Docker Compose is used for managing multi-container applications. It allows you to define and orchestrate multiple containers, their configurations, networks, and volumes using a declarative YAML file called a Compose file. With Docker Compose, you can define the desired state of your application and manage it as a single unit. Compose simplifies the process of running complex applications with multiple interconnected services, making it easier to replicate and share application environments across different environments.</li> </ol> <p>To run a Dockerfile with Docker compose, we use</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker compose up
</code></pre></div></div> <ol> <li><code class="language-plaintext highlighter-rouge">docker run</code> command: The <code class="language-plaintext highlighter-rouge">docker run</code> command is used to run a single container. It allows you to start a container from a specific Docker image with specific configurations. You can specify various options such as environment variables, exposed ports, volume mounts, and networking settings when running a container with <code class="language-plaintext highlighter-rouge">docker run</code>. The <code class="language-plaintext highlighter-rouge">docker run</code> command is typically used for running individual containers in isolation rather than managing complex multi-container applications.</li> </ol> <p>To run with docker run, we use</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run &lt;IMAGE_NAME&gt;:&lt;TAG&gt;
</code></pre></div></div> <h4 id="613-docker-compose-usage">6.1.3 Docker compose usage</h4> <p><a id="s6.1.3-docker-compose-usage"></a></p> <ul> <li>Add <code class="language-plaintext highlighter-rouge">docker-compose.yml</code> in the same directory of your <code class="language-plaintext highlighter-rouge">Dockerfile</code></li> </ul> <p>Sample <code class="language-plaintext highlighter-rouge">docker-compose.yml</code> file:</p> <pre><code class="language-YAML">version: "1"                        #   Specifies the version of Docker Compose syntax being used.
services:                           #   Defines the services within the Docker Compose file.
  deployment:                       #   Represents the first service name
    image: &lt;IMAGE_NAME&gt;:$VERSION  #   Specifies the Docker image to be used for the "deployment" service. 
                                    #   It uses a variable $VERSION to dynamically specify the image version.
    build:                          #   Configures the build settings for the container.            
      dockerfile: Dockerfile        #   It specifies the Dockerfile to be used for building the container. 
                                    #   It assumes there is a file named Dockerfile in the same directory.
    ports:                          #   Maps ports between the container and the host machine.
      - "5001:5001"                 #   Binds port 5001 of the container to port 5001 of the host machine.
    volumes:                        #   Mounts directories or files from the host machine to the container.
      - .:/app    #   Mounts the current directory (denoted by .) to the /app directory inside the container.
    environment:                    #   Mounts directories or files from the host machine to the container. 
      PORT: 5001                    #   Sets the environment variable PORT to the value 5001.
      AUTHOR: "Christopher Le"      #   Sets the environment variable AUTHOR to the value "Christopher Le".

</code></pre> <ul> <li>Build the image first by running <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run &lt;IMAGE_NAME&gt;:<span class="nv">$VERSION</span>
</code></pre></div> </div> </li> <li>Run docker compose by</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker compose up
</code></pre></div></div> <p>To disable docker compose, run</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker compose down
</code></pre></div></div> <h3 id="62-running-docker-image-with-kubernetes">6.2 Running Docker image with Kubernetes</h3> <p><a id="s6.2-run-docker-with-kubernetes"></a></p> <h4 id="621-what-is-kubernetes">6.2.1 What is Kubernetes</h4> <p><a id="s6.2.1-what-is-kubernetes"></a></p> <p>Kubernetes is an open-source container orchestration platform developed by Google. It automates the deployment, scaling, and management of containerized applications. With Kubernetes, you can easily manage and coordinate multiple containers that run across a cluster of servers.</p> <h4 id="622-running-public-docker-image-with-kubernetes">6.2.2 Running public Docker image with Kubernetes</h4> <p><a id="s6.2.2-run-public-docker-image-with-kubernetes"></a></p> <ol> <li><strong>Build the Docker Image:</strong> Build the Docker image using the Dockerfile. Run the following command in the directory containing the Dockerfile:</li> </ol> <div class="language-perl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">docker</span> <span class="nv">build</span> <span class="o">-</span><span class="nv">t</span> <span class="k">my</span><span class="o">-</span><span class="nv">python</span><span class="o">-</span><span class="nv">app</span> <span class="o">.</span>
</code></pre></div></div> <ol> <li><strong>Push the Docker Image:</strong> Push the built image to a container registry of your choice, such as Docker Hub or a private registry. This step is necessary if you want to deploy the image from the registry using Kubernetes. Here’s an example command to push the image to Docker Hub:</li> </ol> <div class="language-perl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">docker</span> <span class="nb">push</span> <span class="nv">your</span><span class="o">-</span><span class="nv">docker</span><span class="o">-</span><span class="nv">username</span><span class="o">/</span><span class="k">my</span><span class="o">-</span><span class="nv">python</span><span class="o">-</span><span class="nv">app</span>
</code></pre></div></div> <ol> <li><strong>Create Kubernetes Configurations:</strong></li> </ol> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-app-deployment</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">my-app</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">my-app</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">my-app-container</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">your-docker-username/my-python-app</span>
          <span class="na">ports</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">5001</span>
          <span class="na">env</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">PORT</span>
              <span class="na">value</span><span class="pi">:</span> <span class="s2">"</span><span class="s">5001"</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">AUTHOR</span>
              <span class="na">value</span><span class="pi">:</span> <span class="s2">"</span><span class="s">Christopher</span><span class="nv"> </span><span class="s">Le"</span>
</code></pre></div></div> <ol> <li><strong>Deploy the Kubernetes Objects:</strong> Apply the Kubernetes configuration by running the following command:</li> </ol> <div class="language-perl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">kubectl</span> <span class="nv">apply</span> <span class="o">-</span><span class="nv">f</span> <span class="k">my</span><span class="o">-</span><span class="nv">app</span><span class="o">-</span><span class="nv">deployment</span><span class="o">.</span><span class="nv">yaml</span>
</code></pre></div></div> <ol> <li><strong>Accessing the Application:</strong> By default, the application is accessible within the Kubernetes cluster. To expose it externally, you can create a Kubernetes service. Create a YAML file (e.g., <code class="language-plaintext highlighter-rouge">my-app-service.yaml</code>) with the following contents:</li> </ol> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-app-service</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">my-app</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
      <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">5001</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">LoadBalancer</span>
</code></pre></div></div> <p>This configuration creates a LoadBalancer service that exposes the application on port 80. Apply the service by running the following command:</p> <div class="language-perl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">kubectl</span> <span class="nv">apply</span> <span class="o">-</span><span class="nv">f</span> <span class="k">my</span><span class="o">-</span><span class="nv">app</span><span class="o">-</span><span class="nv">service</span><span class="o">.</span><span class="nv">yaml</span>
</code></pre></div></div> <p>Once the service is created, you can access your application using the external IP address assigned to the service. Open a web browser and visit <code class="language-plaintext highlighter-rouge">http://&lt;external-ip&gt;:80</code> to interact with your application.</p> <h4 id="623-running-private-docker-image-with-kubernetes">6.2.3 Running private Docker image with Kubernetes</h4> <p><a id="s6.2.3-run-private-docker-image-with-kubernetes"></a></p> <ol> <li><strong>Create a Secret:</strong> Kubernetes provides a mechanism called Secrets to store sensitive information securely, such as credentials for accessing private container registries. Create a Secret to store the authentication details required to pull the private Docker image. Run the following command, replacing the placeholders with your registry credentials:</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create secret docker-registry &lt;secret-name&gt; <span class="nt">--docker-server</span><span class="o">=</span>&lt;registry-server&gt; <span class="nt">--docker-username</span><span class="o">=</span>&lt;username&gt; <span class="nt">--docker-password</span><span class="o">=</span>&lt;password&gt; <span class="nt">--docker-email</span><span class="o">=</span>&lt;email&gt;
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">&lt;secret-name&gt;</code> is the name you choose for the Secret.</li> <li><code class="language-plaintext highlighter-rouge">&lt;registry-server&gt;</code> is the URL of your private container registry.</li> <li><code class="language-plaintext highlighter-rouge">&lt;username&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;password&gt;</code>, and <code class="language-plaintext highlighter-rouge">&lt;email&gt;</code> are the credentials required to access the registry.</li> </ul> <ol> <li><strong>Update the Kubernetes Deployment:</strong> Modify your Kubernetes Deployment configuration to use the created Secret for authentication. Add the following section to your Deployment configuration:</li> </ol> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec</span><span class="pi">:</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">imagePullSecrets</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">&lt;secret-name&gt;</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">&lt;secret-name&gt;</code> should match the name you provided when creating the Secret in the previous step.</p> <ol> <li><strong>Apply the Changes:</strong> Apply the updated Deployment configuration using the kubectl apply command:</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-f</span> &lt;deployment-file&gt;.yaml
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">&lt;deployment-file&gt;.yaml</code> is the path to your Deployment configuration file.</p> <ol> <li><strong>Kubernetes Pulls Private Image:</strong> With the Secret configured in the Deployment, Kubernetes will use the provided credentials to authenticate with your private container registry and pull the image when deploying the application.</li> </ol> <p>Ensure that the Kubernetes cluster where you are deploying your application has network access to the private container registry. If the registry is within a private network, you may need to configure additional networking or authentication mechanisms to establish the connection.</p> <h4 id="624-running-docker-image-with-minikube">6.2.4 Running Docker image with Minikube</h4> <p><a id="s6.2.4-run-docker-image-with-minikube"></a></p> <p><strong>Minikube</strong> is a lightweight Kubernetes implementation that creates a VM on your local machine and deploys a simple cluster containing only one node. Minikube is available for Linux, macOS, and Windows systems.</p> <ol> <li><strong>Start Minikube:</strong> Start Minikube on your local machine by running the following command:</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>minikube start
</code></pre></div></div> <ol> <li><strong>Build the Docker Image:</strong> Build the Docker image for your application using the Dockerfile. Navigate to the directory containing the Dockerfile and run the following command:</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker build <span class="nt">-t</span> my-app-image <span class="nb">.</span>
</code></pre></div></div> <ol> <li><strong>Load Docker Image into Minikube:</strong> Load the Docker image into the Minikube environment by running the following command:</li> </ol> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">eval</span> <span class="si">$(</span>minikube docker-env<span class="si">)</span>
docker image load <span class="nt">-i</span> my-app-image.tar
</code></pre></div></div> <ol> <li><strong>Create Kubernetes Deployment:</strong> Create a Kubernetes Deployment configuration file (e.g., <code class="language-plaintext highlighter-rouge">my-app-deployment.yaml</code>) to define the deployment of your application. Here’s an example configuration for a simple deployment:</li> </ol> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-app-deployment</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">my-app</span>
  <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">app</span><span class="pi">:</span> <span class="s">my-app</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">my-app-container</span>
          <span class="na">image</span><span class="pi">:</span> <span class="s">my-app-image</span>
          <span class="na">ports</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">80</span>
</code></pre></div></div> <ol> <li><strong>Apply the Deployment:</strong> Apply the Deployment configuration to create the deployment in Minikube by running the following command:</li> </ol> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">kubectl apply -f my-app-deployment.yaml
</span></code></pre></div></div> <ol> <li><strong>Expose the Deployment:</strong> To access your application, you need to expose it outside the cluster. In Minikube, you can create a NodePort service to expose the deployment. Create a service configuration file (e.g., <code class="language-plaintext highlighter-rouge">my-app-service.yaml</code>) with the following contents:</li> </ol> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-app-service</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">my-app</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">NodePort</span>
  <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
      <span class="na">targetPort</span><span class="pi">:</span> <span class="m">80</span>
      <span class="na">nodePort</span><span class="pi">:</span> <span class="m">30000</span>
</code></pre></div></div> <p>This configuration creates a NodePort service that exposes the application on <code class="language-plaintext highlighter-rouge">port 30000</code>. Apply the service by running the following command:</p> <div class="language-perl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">kubectl</span> <span class="nv">apply</span> <span class="o">-</span><span class="nv">f</span> <span class="k">my</span><span class="o">-</span><span class="nv">app</span><span class="o">-</span><span class="nv">service</span><span class="o">.</span><span class="nv">yaml</span>
</code></pre></div></div> <ol> <li><strong>Access the Application:</strong> To access your application, you can use the Minikube IP and the NodePort assigned to the service. Run the following command to get the Minikube IP:</li> </ol> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">minikube ip
</span></code></pre></div></div> <p>Remember to clean up the resources when you’re done by running <code class="language-plaintext highlighter-rouge">kubectl delete deployment my-app-deployment</code> and <code class="language-plaintext highlighter-rouge">kubectl delete service my-app-service</code>. Additionally, you can stop Minikube by running <code class="language-plaintext highlighter-rouge">minikube stop</code>.</p> <h3 id="63-add-load-balancer-with-nginx">6.3 Add load balancer with Nginx</h3> <p><a id="s6.3-add-load-balancer-with-nginx"></a></p> <h4 id="631-what-is-load-balancer">6.3.1 What is load balancer</h4> <p><a id="s6.3.1-what-is-load-balancer"></a></p> <p>A load balancer is a networking device or software component that evenly distributes incoming network traffic across multiple servers or resources. It acts as a central point of contact for client requests and forwards those requests to the appropriate backend servers based on predefined rules or algorithms.</p> <h4 id="632-ngnix">6.3.2 Ngnix</h4> <p><a id="s6.3.2-ngnix"></a></p> <p><strong>Nginx</strong> (pronounced “engine-x”) is a popular open-source web server and reverse proxy server. It is known for its high performance, scalability, and ability to efficiently handle concurrent connections. Nginx is designed to handle a large number of client requests while consuming fewer resources compared to traditional web servers.</p> <p><strong>Nginx</strong> is often used as a load balancer alongside its web server capabilities. It can be configured as a reverse proxy and load balancer to distribute incoming traffic across multiple backend servers. This makes Nginx a versatile tool for managing and scaling web applications.</p> <h4 id="633-add-load-balancer-to-docker-compose">6.3.3 Add load balancer to Docker compose</h4> <p><a id="s6.3.3-add-load-balancer-to-docker-compose"></a></p> <p>To add a load balancer to a Docker Compose configuration, you can use a reverse proxy server, such as Nginx or HAProxy, to distribute incoming traffic across multiple containers running your application. Here’s an example of how you can add Nginx as a load balancer in a Docker Compose file:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">version</span><span class="pi">:</span> <span class="s1">'</span><span class="s">3'</span>

<span class="na">services</span><span class="pi">:</span>
  <span class="na">nginx</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">nginx:latest</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">80:80</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">./nginx.conf:/etc/nginx/nginx.conf</span>
    <span class="na">depends_on</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">app1</span>
      <span class="pi">-</span> <span class="s">app2</span>

  <span class="na">app1</span><span class="pi">:</span>
    <span class="na">build</span><span class="pi">:</span>
      <span class="na">context</span><span class="pi">:</span> <span class="s">./app1</span>
    <span class="c1"># Other configurations for your app1 container</span>

  <span class="na">app2</span><span class="pi">:</span>
    <span class="na">build</span><span class="pi">:</span>
      <span class="na">context</span><span class="pi">:</span> <span class="s">./app2</span>
    <span class="c1"># Other configurations for your app2 container</span>
</code></pre></div></div> <p>In the above example, we have three services defined: <code class="language-plaintext highlighter-rouge">nginx</code>, <code class="language-plaintext highlighter-rouge">app1</code>, and <code class="language-plaintext highlighter-rouge">app2</code>. The nginx service uses the official Nginx image and maps <code class="language-plaintext highlighter-rouge">port 80</code> of the host to port 80 of the Nginx container. The volumes section mounts a custom <code class="language-plaintext highlighter-rouge">nginx.conf</code> file, which we’ll create next.</p> <p>Create an <code class="language-plaintext highlighter-rouge">nginx.conf</code> file in the same directory as the Docker Compose file with the following content:</p> <div class="language-conf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">events</span> {}

<span class="n">http</span> {
  <span class="n">upstream</span> <span class="n">app_servers</span> {
    <span class="n">server</span> <span class="n">app1</span>:<span class="m">5000</span>;
    <span class="n">server</span> <span class="n">app2</span>:<span class="m">5000</span>;
  }

  <span class="n">server</span> {
    <span class="n">listen</span> <span class="m">80</span>;
    <span class="n">server_name</span> <span class="n">localhost</span>;

    <span class="n">location</span> / {
      <span class="n">proxy_pass</span> <span class="n">http</span>://<span class="n">app_servers</span>;
      <span class="n">proxy_set_header</span> <span class="n">Host</span> $<span class="n">host</span>;
      <span class="n">proxy_set_header</span> <span class="n">X</span>-<span class="n">Real</span>-<span class="n">IP</span> $<span class="n">remote_addr</span>;
    }
  }
}
</code></pre></div></div> <p>In the <code class="language-plaintext highlighter-rouge">nginx.conf</code>, we define an upstream block called app_servers, which lists the hostnames and ports of the backend application containers (<code class="language-plaintext highlighter-rouge">app1</code> and <code class="language-plaintext highlighter-rouge">app2</code>). The server block sets up the Nginx configuration to listen on <code class="language-plaintext highlighter-rouge">port 80</code> and forward incoming requests to the <code class="language-plaintext highlighter-rouge">app_servers</code> upstream.</p> <p>To start the Docker Compose stack, navigate to the directory containing the Docker Compose file and run the following command:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker compose up
</code></pre></div></div> <p>This will start the Nginx container and the two application containers (<code class="language-plaintext highlighter-rouge">app1</code> and <code class="language-plaintext highlighter-rouge">app2</code>). Incoming traffic will be load balanced by Nginx and forwarded to the backend application containers based on the configuration in <code class="language-plaintext highlighter-rouge">nginx.conf</code>.</p> <p>Make sure to adjust the build configurations for <code class="language-plaintext highlighter-rouge">app1</code> and <code class="language-plaintext highlighter-rouge">app2</code> according to your application’s needs. Also, update the port numbers and other settings as required for your specific use case.</p> <p>Note: This configuration assumes that your backend application containers (<code class="language-plaintext highlighter-rouge">app1</code> and <code class="language-plaintext highlighter-rouge">app2</code>) are configured to listen on <code class="language-plaintext highlighter-rouge">port 5000</code>. Adjust the upstream server addresses accordingly if your containers use different ports.</p> <hr/> <h2 id="7-contributing">7. Contributing</h2> <p><a id="s7-contributing"></a></p> <h3 id="71-contributing-guide"><a href="https://github.com/chrislevn/dockerfile-practices/blob/main/CONTRIBUTING.md">7.1 Contributing guide</a></h3> <p><a id="s7.1-contributing-guide"></a></p> <h3 id="72-acknowledgement">7.2 Acknowledgement</h3> <p><a id="s7.2-acknowledgement"></a></p> <ul> <li><a href="https://github.com/ducthinh993">Thinh Nguyen</a> - 3.4, 3.5</li> </ul> <h2 id="8-references">8. References:</h2> <p><a id="s8-references"></a></p> <ul> <li><a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/">https://docs.docker.com/develop/develop-images/dockerfile_best-practices/</a></li> <li><a href="https://medium.com/@adari.girishkumar/dockerfile-and-best-practices-for-writing-dockerfile-diving-into-docker-part-5-5154d81edca4">https://medium.com/@adari.girishkumar/dockerfile-and-best-practices-for-writing-dockerfile-diving-into-docker-part-5-5154d81edca4</a></li> <li><a href="https://google.github.io/styleguide/pyguide.html">https://google.github.io/styleguide/pyguide.html</a></li> <li><a href="https://github.com/dnaprawa/dockerfile-best-practices">https://github.com/dnaprawa/dockerfile-best-practices</a></li> <li><a href="https://we-are.bookmyshow.com/understanding-expose-in-dockerfile-266938b6a33d">https://we-are.bookmyshow.com/understanding-expose-in-dockerfile-266938b6a33d</a></li> <li><a href="https://towardsdatascience.com/goodbye-pip-freeze-welcome-pipreqs-258d2e7a5a62">https://towardsdatascience.com/goodbye-pip-freeze-welcome-pipreqs-258d2e7a5a62</a></li> <li><a href="https://www.knowledgehut.com/blog/devops/docker-vs-container">https://www.knowledgehut.com/blog/devops/docker-vs-container</a></li> <li><a href="https://refine.dev/blog/docker-build-args-and-env-vars/#how-to-pass-arg-variables">https://refine.dev/blog/docker-build-args-and-env-vars/#how-to-pass-arg-variables</a></li> </ul>]]></content><author><name></name></author><category term="devops"/><category term="docker"/><summary type="html"><![CDATA[&lt;!– Copyright [2023] [Christopher Le]]]></summary></entry><entry><title type="html">Top backend languages and when to use them</title><link href="https://christopherle.com//blog/2023/backend-languages/" rel="alternate" type="text/html" title="Top backend languages and when to use them"/><published>2023-11-11T23:35:00+00:00</published><updated>2023-11-11T23:35:00+00:00</updated><id>https://christopherle.com//blog/2023/backend-languages</id><content type="html" xml:base="https://christopherle.com//blog/2023/backend-languages/"><![CDATA[<blockquote> <p>This is a continuation of my <a href="https://christopherle.com/blog/2023/backend-git/">previous post</a> on my takeaways of Git. Following <a href="https://roadmap.sh/backend">this back-end basic roadmap</a>, I will be going over the basics of backend languages in today post. I won’t be covering everything, just my takeaways of what I think is valuable.</p> </blockquote> <p>When it comes to Backend Web Development – we primarily require a backend (or you can say server-side) programming language to make the website function along with various other tools &amp; technologies such as databases, frameworks, web servers, etc.</p> <p>Pick a language from the given list and make sure to learn its quirks, core details about its concurrency, memory model, runtime etc.</p> <h2 id="basics-of-concurrency-memory-model-and-runtime">Basics of concurrency, memory model, and runtime:</h2> <h3 id="concurrency">Concurrency</h3> <p>Concurrency refers to the ability of multiple threads to access shared resources simultaneously. Concurrency means happening at (about) the same time. As opposed to happening in parallel, truly.</p> <h2 id="comparison-of-backend-languages">Comparison of backend languages</h2> <table> <thead> <tr> <th>Language</th> <th>Concurrency</th> <th>Runtime</th> <th>Memory Model</th> <th>Use Cases</th> </tr> </thead> <tbody> <tr> <td>Python</td> <td>Multi-threading, asyncio</td> <td>Interpreted, CPython, PyPy</td> <td>Dynamic, managed</td> <td>Web applications, Data analysis, AI/ML, Scripting</td> </tr> <tr> <td>Rust</td> <td>Async, multi-threading, Actor model</td> <td>Compiled</td> <td>Ownership, Borrowing</td> <td>System programming, WebAssembly, Embedded systems</td> </tr> <tr> <td>Go</td> <td>Goroutines, channels</td> <td>Compiled, Go runtime</td> <td>Garbage collected</td> <td>Network services, Concurrent processing</td> </tr> <tr> <td>PHP</td> <td>Multi-threading (limited), async (libraries)</td> <td>Interpreted, Zend Engine</td> <td>Dynamic, managed</td> <td>Web development, CMS, E-commerce platforms</td> </tr> <tr> <td>JavaScript</td> <td>Event loop, async/await</td> <td>Interpreted, Node.js runtime</td> <td>Dynamic, managed</td> <td>Web development, Server-side applications</td> </tr> <tr> <td>Java</td> <td>Threads, Future, CompletableFuture</td> <td>Compiled to bytecode, JVM</td> <td>Garbage collected</td> <td>Enterprise applications, Android apps, Web services</td> </tr> <tr> <td>C#</td> <td>Async/await, Task Parallel Library</td> <td>Compiled to bytecode, CLR</td> <td>Garbage collected</td> <td>Web applications, Desktop applications, Games</td> </tr> <tr> <td>Ruby</td> <td>Threads, Fibers</td> <td>Interpreted, YARV</td> <td>Dynamic, managed</td> <td>Web applications (Rails), Scripting, Prototyping</td> </tr> </tbody> </table> <p><br/> Let’s look at some of the criteria to considered:</p> <h3 id="concurrency-1">Concurrency:</h3> <ul> <li>Python and Ruby have limitations in concurrency due to GIL (Global Interpreter Lock), though they support multi-threading and have asynchronous libraries.</li> <li>Rust and Go are known for their efficient concurrency models. Rust uses advanced concepts like ownership and borrowing, while Go uses goroutines and channels.</li> <li>Java, C#, and JavaScript (Node.js) support asynchronous programming and are quite capable in handling concurrent processes.</li> </ul> <h3 id="runtime">Runtime:</h3> <ul> <li>Python, PHP, JavaScript, and Ruby are interpreted languages with dynamic memory management, often leading to slower runtime performance compared to compiled languages. <blockquote> <p>Python: source code -&gt; interpreter -&gt; operating system -&gt; hardware.</p> </blockquote> </li> <li>Rust, Go, Java, and C# have compiled runtimes, offering generally better performance. Java and C# run on virtual machines (JVM and CLR), which offer cross-platform support. <blockquote> <p>Java: bytecode -&gt; virtual machine -&gt; machine code -&gt; operating system -&gt; hardware.</p> </blockquote> </li> </ul> <blockquote> <p>C++: machine code -&gt; operating system -&gt; hardware.</p> </blockquote> <h3 id="memory-model">Memory Model:</h3> <ul> <li>Rust has a unique memory model focusing on safety without a garbage collector.</li> <li>Go, Java, and C# use garbage collection to manage memory, balancing performance and ease of use.</li> <li>Python, PHP, JavaScript, and Ruby have dynamic memory models with managed garbage collection, which simplifies development at the cost of some control and efficiency.</li> </ul> <h3 id="use-cases">Use Cases:</h3> <ul> <li>Python and Ruby are often chosen for their ease of use and rapid development capabilities, especially in web applications and scripting.</li> <li>ust is gaining popularity for system-level programming and scenarios where safety and performance are critical.</li> <li>Go is preferred for networked applications and services requiring high concurrency.</li> <li>PHP remains a staple for traditional web development.</li> <li>JavaScript (Node.js) is ubiquitous in modern web development.</li> <li>Java and C# are used extensively in enterprise environments for their robustness and scalability.</li> </ul> <h2 id="other-criteria-to-consider-include">Other criteria to consider include:</h2> <table> <thead> <tr> <th>Language</th> <th>Targeted Platform</th> <th>Elasticity of Language</th> <th>Time to Production</th> <th>Performance</th> <th>Support and Community</th> </tr> </thead> <tbody> <tr> <td>Python</td> <td>Cross-platform, web, data science</td> <td>High (Dynamic typing)</td> <td>Fast</td> <td>Lower (interpreted)</td> <td>Very large, widespread use</td> </tr> <tr> <td>Rust</td> <td>Cross-platform, system, embedded, WebAssembly</td> <td>Low (Strict type system)</td> <td>Slower</td> <td>Very high (compiled)</td> <td>Growing, strong backing by Mozilla</td> </tr> <tr> <td>Go</td> <td>Cross-platform, server-side, cloud</td> <td>Moderate (Balanced simplicity)</td> <td>Quick</td> <td>High (compiled, efficient for concurrency)</td> <td>Growing, strong backing by Google</td> </tr> <tr> <td>PHP</td> <td>Web development</td> <td>High (Dynamic, flexible)</td> <td>Fast</td> <td>Moderate (interpreted)</td> <td>Large, especially in web development</td> </tr> <tr> <td>JavaScript</td> <td>Web, server-side (Node.js), mobile (React Native)</td> <td>High (Dynamic, flexible)</td> <td>Fast</td> <td>Good (V8 engine optimizations)</td> <td>Very large, diverse applications</td> </tr> <tr> <td>Java</td> <td>Cross-platform, enterprise, Android</td> <td>Moderate (Strong typing, flexible frameworks)</td> <td>Moderate</td> <td>Good (JVM optimizations)</td> <td>Very large, enterprise focus</td> </tr> <tr> <td>C#</td> <td>Cross-platform (.NET Core), desktop, web, mobile, games</td> <td>Moderate (Strong typing, flexible frameworks)</td> <td>Moderate</td> <td>Good (CLR optimizations)</td> <td>Large, strong backing by Microsoft</td> </tr> <tr> <td>Ruby</td> <td>Cross-platform, web (Ruby on Rails)</td> <td>High (Dynamic typing)</td> <td>Fast</td> <td>Lower (interpreted)</td> <td>Dedicated, particularly around Rails</td> </tr> </tbody> </table> <p><br/></p> <h4 id="notes">Notes:</h4> <p>I didn’t make comparison in terms of speed due to the fact that it’s hard to compare languages. It’s better to compare the speed of a specific task. Also, although it can be faster to run a specific task, would it be faster to develop considering the learning curve and current codebase?</p> <p>It’s also worth noting that:</p> <ul> <li>the performance of a language is not the only factor that affects the performance of a web application. The performance of a web application is also affected by the performance of the web server, the database, the network, and the client-side code.</li> <li>programming languages are always updated and improved. For example, Python 3.10 is faster than Python 3.9 due to faster method calls and more efficient memory management. There are always pros and cons. Don’t learn everything, just learn what you need for your use cases and master it.</li> </ul> <hr/> <p>One note about Python and Mojo. As I said, languages change over time. Overcome the limitations of Python: Python is known for its slow speed. However, I just got to know about Mojo</p> <blockquote> <p>Mojo combines the usability of Python with the performance of C, unlocking unparalleled programmability of AI hardware and extensibility of AI models.</p> </blockquote> <p><img src="https://miro.medium.com/v2/resize:fit:942/1*GamnxUBaLlHbSKoOdbEK7g.png" alt="Sample code with Mojo"/></p> <p>The reason why Mojo is fast is Mojo is compiled to machine code, while Python is interpreted. This means that Mojo code can be executed directly by the CPU, while Python code must first be translated into an intermediate form called bytecode. This translation process adds to the overhead of executing Python code. I will cover another in-depth post about Mojo in the future.</p> <p>Read more about Mojo <a href="https://www.modular.com/mojo">here</a>.</p> <p>How Mojo is 35,000x faster than Python? Read more <a href="https://www.modular.com/blog/how-mojo-gets-a-35-000x-speedup-over-python-part-1">here</a>.</p> <hr/> <p>In the next post, I will talk about the basics of CPU (cocurrency, I/O, memory management, etc)</p> <h1 id="references">References</h1> <ul> <li>https://www.makeuseof.com/build-http-web-server-in-rust/</li> <li>https://discourse.julialang.org/t/why-mojo-can-be-so-fast/98458</li> <li>https://www.fast.ai/posts/2023-05-03-mojo-launch.html</li> <li>https://blog.logrocket.com/when-to-use-rust-when-to-use-golang/</li> <li>https://www.freecodecamp.org/news/what- is-java-used-for/</li> <li>https://data-flair.training/blogs/pros-and-cons-of-java/</li> </ul>]]></content><author><name></name></author><category term="backend"/><category term="languages"/><category term="python"/><category term="rust"/><category term="go"/><summary type="html"><![CDATA[This is a continuation of my previous post on my takeaways of Git. Following this back-end basic roadmap, I will be going over the basics of backend languages in today post. I won’t be covering everything, just my takeaways of what I think is valuable.]]></summary></entry><entry><title type="html">My takeaways of Git</title><link href="https://christopherle.com//blog/2023/backend-git/" rel="alternate" type="text/html" title="My takeaways of Git"/><published>2023-11-10T23:35:00+00:00</published><updated>2023-11-10T23:35:00+00:00</updated><id>https://christopherle.com//blog/2023/backend-git</id><content type="html" xml:base="https://christopherle.com//blog/2023/backend-git/"><![CDATA[<blockquote> <p>This is a continuation of my <a href="https://christopherle.com/blog/2023/inside-browser/">previous post</a> on what happend when you type a URL into a browser. Following <a href="https://roadmap.sh/backend">this back-end basic roadmap</a>, I will be going over the basics of Git in today post. I won’t be covering everything, just my takeaways of what I think is valuable.</p> </blockquote> <h2 id="1-what-is-git">1. What is Git?</h2> <p>Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency.</p> <h2 id="2-how-git-works">2. How Git works?</h2> <p>Unlike other VCSs, Git takes a snapshot of your file every time you make a commit. Git stores these snapshots as a reference to the previous commit.</p> <p><img src="https://git-scm.com/book/en/v2/images/snapshots.png" alt="Figure 5. Storing data as snapshots of the project over time"/></p> <p>Git also have branches and almost every operation is local. Everything in Git is also checksummed (SHA-1) before it is stored and is then referred to by that checksum. Meaning it is impossible to change the contents of any file or directory without Git knowing about it. This functionality is built into Git at the lowest levels and is integral to its philosophy.</p> <h2 id="3-why-using-git">3. Why using Git?</h2> <blockquote> <p>When you do actions in Git, nearly all of them only add data to the Git database. It is hard to get the system to do anything that is not undoable or to make it erase data in any way</p> </blockquote> <p>That means you can always undo any action in Git, making it easy to experiment with different ideas, with others, without worrying about losing your original work.</p> <h2 id="4-my-takeaways-of-git">4. My takeaways of Git</h2> <p>I won’t cover everything about Git, just my takeaways of what I think is valuable.</p> <h3 id="41-dont-use-git-merge">4.1. Don’t use git merge</h3> <p>During my time working with backend, I found it is useful to learn how to make a pull request, merge, and rebase. As an engineer, you should also know how to make linear commit history.</p> <p><img src="https://www.bitsnbites.eu/wp-content/uploads/2015/12/1-nonlinear-vs-linear.png" alt="Image by Bits'n'Bites"/></p> <p>It’s important not to use <code class="language-plaintext highlighter-rouge">git merge</code> when you are working on a feature branch. Instead, use <code class="language-plaintext highlighter-rouge">git rebase</code>. This will make your commit history linear. The reason is that <code class="language-plaintext highlighter-rouge">git merge</code> will create a new commit that combines the two branches. This will make your commit history look messy.</p> <p>Here is how I did it.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git checkout master
git pull <span class="nt">--rebase</span>
git checkout &lt;branch_name&gt;
git rebase origin/master
git push <span class="nt">-f</span> origin &lt;branch_name&gt;
</code></pre></div></div> <h3 id="42-make-git-commits-small-and-atomic">4.2 Make git commits small and atomic</h3> <p>A commit should be a single logical change. Don’t make several logical changes in one commit. For example, if a patch fixes a bug and optimizes the performance of a feature, split it into two separate commits.</p> <p>Sometimes, we will do a bunch of testing commits since we can’t test the system locally. It’s important to squash these commits into one commit before making a pull request, making easier for reviewers to review your code.</p> <h3 id="43-identify-a-branching-strategy">4.3 Identify a branching strategy</h3> <p>While there are several approaches to development, the most common are:</p> <p>Centralized workflow: Teams use only a single repository and commit directly to the main branch. Centralized workflow is a good approach for small teams and projects.</p> <p>Feature branching: Teams use a new branch for each feature and don’t commit directly to the main branch. It’s a good approach for large teams and projects. Feature branching is the most common branching strategy. A branch with feature is usually named <code class="language-plaintext highlighter-rouge">feature/&lt;feature_name&gt;</code>.</p> <p>GitFlow: An extreme version of feature branching in which development occurs on the develop branch, moves to a release branch, and merges into the main branch. GitFlow is a good approach for large teams and projects.</p> <p>Personal branching: Similar to feature branching, but rather than develop on a branch per feature, it’s per developer. Every user merges to the main branch when they complete their work. Personal branching is a good approach for small teams and projects.</p> <h2 id="44-5-steps-to-write-meaningful-commit-messages">4.4 5 steps to write meaningful commit messages</h2> <ul> <li> <p>Capitalization and Punctuation: Capitalize the first word and do not end in punctuation. If using Conventional Commits, remember to use all lowercase.</p> </li> <li>Mood: Use imperative mood in the subject line. Example: <ul> <li>Add fix for dark mode toggle state. Imperative mood gives the tone you are giving an order or request.</li> </ul> </li> <li>Type of Commit: Specify the type of commit. It is recommended and can be even more beneficial to have a consistent set of words to describe your changes. Example: <ul> <li>Bugfix, Update, Refactor, Bump, and so on. See the section on Conventional Commits below for additional information.</li> </ul> </li> <li> <p>Length: The first line should ideally be no longer than 50 characters, and the body should be restricted to 72 characters.</p> </li> <li>Content: Be direct, try to eliminate filler words and phrases in these sentences (examples: though, maybe, I think, kind of). Think like a journalist.</li> </ul> <h2 id="references">References</h2> <ul> <li>“1.3 Getting Started - What Is Git?” Git, git-scm.com/book/en/v2/Getting-Started-What-is-Git%3F. Accessed 26 Nov. 2023.</li> <li>“What Are Git Version Control Best Practices?” GitLab, GitLab, about.gitlab.com/topics/version-control/version-control-best-practices/. Accessed 26 Nov. 2023.</li> </ul>]]></content><author><name></name></author><category term="backend"/><category term="git"/><summary type="html"><![CDATA[This is a continuation of my previous post on what happend when you type a URL into a browser. Following this back-end basic roadmap, I will be going over the basics of Git in today post. I won’t be covering everything, just my takeaways of what I think is valuable.]]></summary></entry><entry><title type="html">My current understanding of MLOps workflow</title><link href="https://christopherle.com//blog/2023/current-understanding-mlops/" rel="alternate" type="text/html" title="My current understanding of MLOps workflow"/><published>2023-07-21T13:56:00+00:00</published><updated>2023-07-21T13:56:00+00:00</updated><id>https://christopherle.com//blog/2023/current-understanding-mlops</id><content type="html" xml:base="https://christopherle.com//blog/2023/current-understanding-mlops/"><![CDATA[<h2 id="my-current-understanding-of-mlops-workflow">My current understanding of MLOps workflow</h2> <p>This series will be about how to deploy your AI model from A to Z. Everything is based on my current understanding and learning of Machine Learning, Cloud engineering, and Full-stack development.</p> <p><strong>Why I created this blog?</strong></p> <p>As one of the ML self-taught, I have created many Machine Learning models that don’t have an API out there yet. However, none went to production. In spite of the non-technical challenges, it is hard for a data scientist to scale their Machine Learning model without the help of an ML Engineer/Software Engineer/DevOps Engineer (read more <a href="https://towardsdatascience.com/why-90-percent-of-all-machine-learning-models-never-make-it-into-production-ce7e250d5a4a">here</a>). This whole workflow of deploying ML models lay in the category of MLOps (Machine Learning Operations). That’s why I was inspired to write this blog series. My main goal is to find a deployment workflow that is scaleable and easy. In the end, I hope data scientists can worry less about scaling and focus on improving the accuracy of their models.</p> <p>Note that this first blog is an introduction so there will be no coding tutorial.</p> <p><strong>Who is this blog for?</strong></p> <ul> <li>Machine Learning practitioners/Data scientists who would like to test their ideas in production.</li> </ul> <p>However, this blog is not for a big production or if you prefer on-premise over Cloud workflow. One of the reasons I can think of when using on-premise over Cloud workflow is having sensitive data that some companies are hesitant to store on the Cloud. Another is the lack of full control in response to the convenience of easier deployment.</p> <h2 id="mlops-workflow"><strong>MLOps workflow</strong></h2> <p><img src="https://cdn-images-1.medium.com/max/2000/1*fyda87Cy6_OZIrgK3jmHEQ.png" alt="My current simple understanding of MLOPs workflow"/></p> <ol> <li><strong>Get data</strong></li> </ol> <p>There are many ways to store your data. While on-premise has advantages over control and security, Cloud Data Storage such as GCP, AWS, Azure, and MongoDB offers data storage and infrastructures that you don’t have to worry about maintaining.</p> <p>Remember that you might need data version control to keep track of changes in the dataset. Read more about data version control with DVC from this <a href="https://medium.com/geekculture/data-version-control-dvc-with-google-cloud-storage-and-python-for-ml-fe99dc7d338">blog</a>.</p> <p><img src="https://cdn-images-1.medium.com/max/2000/0*k_33-imUwWHZhv2G.png" alt="*Data management middleware from dvc.org*"/></p> <p><strong>2. Visualize data</strong></p> <p>You can also easily visualize your current data on the Cloud with tools such as BigQuery from GCP, AWS QuickSight, or the Double Cloud (check my previous <a href="https://medium.com/@locvicvn1234/visualize-your-data-with-doublecloud-and-clickhouse-db-8713796389ab?postPublishedType=repub">post</a>). No need to constantly download and visualize new data.</p> <p><img src="https://cdn-images-1.medium.com/max/2000/0*Vo3Rk9uez8kYYAv_" alt="Unsplash: data visualization example"/></p> <p><strong>3. Pre-processing/Clean data</strong></p> <p>Sometimes, data can come in raw like an image, text, sound, video file or as a table (.csv, excel). It’s your job to make these raw data processable for the model that you built.</p> <p>Sometimes, inputting these data can take a ton of time, considering there are many users for your app or the size of the data. Instead of having your notebook doing everything, one way to optimize this process is to have your cleaning process on Cloud using tools such as Vertex AI from GCP. The Cloud will take in your input file and return your processed input.</p> <p><img src="https://cdn-images-1.medium.com/max/2000/0*7WtZCEjTqBAXtN1V.png" alt="Vertex AI logo"/></p> <ol> <li>Training model</li> </ol> <p>Lastly, training a new model, whether it’s completely new or pre-trained, can take lots of time. Having your model trained on Cloud providers will reduce the training time as well as infrastructure maintenance from your side. There are many tools for this step such as IBM Watson Studio, Databricks Lakehouse, GCP Vertex AI, etc.</p> <ol> <li>Deploying your model</li> </ol> <p>Deploying the model is also crucial so that others can make use of and test your model. This post series will focus on making your model an API. If you would like to learn how to have a front end to test your API, consider tools such as <a href="https://streamlit.io/">Streamlit</a>.</p> <p><img src="https://cdn-images-1.medium.com/max/2000/0*dVTeOo1i6Q1zY4NN.png" alt="Sample Streamlit app from streamli.io"/></p> <p>From this step, every time you get new data, it can be added for visualization, processed, and tested on the deployed model.</p> <ol> <li>Model monitor</li> </ol> <p>Finally, model monitoring with tools such as <a href="https://cloud.google.com/vertex-ai/docs/model-monitoring">Vertex AI</a> is the essential step to check the accuracy of your model. It also involves of CI/CD to take in new data to be retrained and deployed on a new model.</p> <p>This is my first post on MLOps. I understand there will be a lot of things I don’t know yet so please give me any feedback. While these steps might sound a lot and vague now, I aim to make the whole process visualized and easier.</p> <p>Until then, take care! See you in the next blog post on how to deploy your ML model.</p>]]></content><author><name></name></author><category term="AI"/><category term="mlops"/><summary type="html"><![CDATA[My current understanding of MLOps workflow]]></summary></entry><entry><title type="html">What happens when you type a URL into your browser? — The big picture (with Cloud)</title><link href="https://christopherle.com//blog/2023/inside-browser/" rel="alternate" type="text/html" title="What happens when you type a URL into your browser? — The big picture (with Cloud)"/><published>2023-05-31T13:56:00+00:00</published><updated>2023-05-31T13:56:00+00:00</updated><id>https://christopherle.com//blog/2023/inside-browser</id><content type="html" xml:base="https://christopherle.com//blog/2023/inside-browser/"><![CDATA[<h2 id="what-happens-when-you-type-a-url-into-your-browser--the-big-picture-with-cloud">What happens when you type a URL into your browser? — The big picture (with Cloud)</h2> <p>Have you ever wondered what happens behind the scenes when you type a URL into your browser? It’s a simple action that we often take for granted, but the journey from entering a web address to seeing a fully-rendered website involves multiple intricate steps. In this blog, we will demystify the process and shed light on the inner workings of the internet, exploring the key stages that occur when you type a URL into your browser. I will keep it as simple as possible with 3 parts:</p> <ul> <li> <p>What is in the URL?</p> </li> <li> <p>What happens when you type a URL into your browser?</p> </li> <li> <p>How we can use cloud services such as AWS/GCP/Azure, Terraform, Kubernetes, and Docker in this example?</p> </li> </ul> <p><img src="https://cdn-images-1.medium.com/max/2000/0*2rAkxbt_vcBiO5Qy.jpg" alt="AfterAcademy"/></p> <h2 id="what-is-in-the-url">What is in the URL?</h2> <p>The URL you entered into the address bar (for example, <a href="http://www.google.com">www.google.com</a> or <a href="https://www.google.com/">https://www.google.com</a>) contains several parts:</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*6y-4y2ygZNPwJUBEv_qHTw.png" alt=""/></p> <h2 id="protocol-https">Protocol (http/s):</h2> <p>The protocol HTTP/S (Hypertext transfer protocol/secure). HTTP is the foundation of World Wide Web and is used to load webpages using hypertext links. HTTP is used to transfer data, such as text, images, and videos, between web servers and web browsers. We use GET and POST requests to get or post content to the desired webpages via HTTP protocol.</p> <p><img src="https://cdn-images-1.medium.com/max/2000/0*6H7KLCpcmSPn_WQe.png" alt="[www3.ntu.edu.sg](http://www3.ntu.edu.sg)"/></p> <p>However, HTTP is a plain text protocol, which means that the data that is transferred between the web browser and the web server is not encrypted. This means that anyone who can intercept the data, such as a hacker, can read it. HTTPS is an extension of HTTP that uses Secure Sockets Layer (SSL) or Transport Layer Security (TLS) to encrypt the data that is transferred between the web browser and the web server. This makes HTTPS much more secure than HTTP.</p> <p><img src="https://cdn-images-1.medium.com/max/2000/0*Arx-OQN-L9hoh7DL.png" alt="Source: SSL Store"/></p> <p>As for SSL and TLS, they work as below:</p> <ol> <li> <p>The client (e.g., a web browser) sends a request to the server.</p> </li> <li> <p>The server sends back a certificate, which contains the server’s public key.</p> </li> <li> <p>The client uses the public key to encrypt a message, which is sent back to the server.</p> </li> <li> <p>The server uses its private key to decrypt the message.</p> </li> <li> <p>If the message is successfully decrypted, then the client and the server can communicate securely.</p> </li> </ol> <p><img src="https://cdn-images-1.medium.com/max/2000/0*olTav7yi-43C2n5M.jpg" alt="Source: MDaemon Blog"/></p> <h2 id="dns-wwwgooglecom">DNS (<a href="http://www.google.com">www.google.com</a>):</h2> <p>Do you know that every single URL like <a href="http://google.com">google.com</a> has a unique IP address. For <a href="http://google.com">google.com</a>, it is 142.251.32.46. So if you enter 142.251.32.46 into your address bar, it will return <a href="http://google.com">google.com</a> site. DNS (Domain Name System) is like a phone book with a list of IP addresses and their corresponding URLs. When you enter an URL into the address bar, DNS will look for the correct IP address.</p> <p>There are 3 parts in the URLs (without HTTP/S):</p> <ul> <li> <p>www (World Wide Web) is a protocol used to access websites that are hosted on the internet.</p> </li> <li> <p>Domain name (google)</p> </li> <li> <p>Top-level domain name (.com)</p> </li> </ul> <p>However, many website URLs we encounter today contain a third-level domain, a second-level domain, and a top-level domain.</p> <ul> <li> <p>Top-level domain are .com, .edu, .gov, etc.</p> </li> <li> <p>Domain name (second-level domain) are google, wikipedia, etc.</p> </li> <li> <p>Third-level domain are <a href="http://map.google.com">map.google.com</a> for example</p> </li> </ul> <p>The root name server will redirect it to the <strong>.com</strong> domain name server. <strong>.com</strong> name server will redirect it to the [<strong>google.com](http://google.com)</strong> name server. The [<strong>google.com](http://google.com)</strong> name server will find the matching IP address for [<strong>maps.google.com](http://maps.google.com)</strong> in its’ DNS records and return it to your DNS recursor, which will send it back to your browser.</p> <p><img src="https://cdn-images-1.medium.com/max/2464/0*t_dRx7pPzk7pc0pb" alt="Source: Red Hat"/></p> <h2 id="what-happens-when-you-type-a-url-into-your-browser">What happens when you type a URL into your browser?</h2> <p><img src="https://cdn-images-1.medium.com/max/2000/1*uGBZcU70veeXbA9LEtQXWw.png" alt=""/></p> <p>Okay, let’s go to the main part. When you enter a URL into the address bar:</p> <ul> <li>DNS will go look up the URL and find the correct IP address. Let’s say the DNS found the IP address for <a href="https://www.google.com">https://www.google.com</a> is 142.251.32.46. This IP address will be return to the client’s computer.</li> </ul> <p><strong>TCP/IP (Transmission Control Protocol):</strong></p> <ul> <li>This communication protocol will use the IP address we found to send the data to us. The client computer opens a TCP connection the server computer at the IP address we found. Then, to put it simple, our client computer sends a request to the server computer and the server computer retrieves the data back to our server computer (the website). That’s how you get all the front-end data (for example, index.html) of the website.</li> </ul> <p>For those curious in how TCP/IP works in-depth, TCP works in the three-way handshake process:</p> <ul> <li> <p>Step 1 (SYN): The client wants to establish a connection with a server so it sends a segment with SYN (Synchronize Sequence number) which inform that the client wants to establish a connection to the server.</p> </li> <li> <p>Step 2 (SYN + ACK): Server responds to to the client request with SYN-ACK signal bits set. ACK (acknowledgment) signifies the response of the segment it received and SYN signifies with what sequence nuymber it is likely to start the segment with. This steps also require the server to have open ports that can accept and initiate new connections. A ports acts as a virtual endpoint that allows multiple services to run on the same device while keeping the communication organized.</p> </li> <li> <p>Step 3 (ACK): Both client and server establish a connection and will begin the data transfer.</p> </li> </ul> <p><img src="https://cdn-images-1.medium.com/max/2000/0*vYFHiTYAdDb47joc.jpg" alt="Source: Wallarm"/></p> <p>This is not to mention TCP/IP Model contains 5 layers:</p> <ul> <li> <p>Application layer (what service you are using: web or email for example)</p> </li> <li> <p>Transport layer (reliable (TCP) or unreliable but faster connection (UDP) — mostly use for live stream, video calls)</p> </li> <li> <p>Network/Internet layer (logical addressing for devices on a network)</p> </li> <li> <p>Data link layer(error detection and correction for data being transmitted over a network)</p> </li> <li> <p>Physical layer (transmits bits over a physical medium such as cable or wireless signal)</p> </li> </ul> <h2 id="https-request">HTTP/S request</h2> <p>Once the TCP connection is established, let’s tranfer data! The browser will send a GET request asking for <a href="http://www.google.com">www.google.com</a> web page. You can also send POST request to websites that requires something like entering credentials or submitting a form.</p> <p>The server, which contains a webserver such as Apache will receives the request from the browser and passes it to a request handler to read and generate a response. The request handler is a program (written in <a href="http://ASP.NET">ASP.NET</a>, PHP, Ruby, etc.) that reads the request, its’ headers, and cookies to check what is being requested and also update the information on the server if needed. Then it will assemble a response in a particular format (JSON, XML, HTML). (Maneesa)</p> <p><strong>HTTP/S response:</strong></p> <p>The server response contains the web page you requested as well as the status code, compression type (<em>Content-Encoding)</em>, how to cache the page (<em>Cache-Control</em>), any cookies to set, privacy information, etc. (Maneesa)</p> <p><img src="https://cdn-images-1.medium.com/max/2000/0*ya9XkMmOWefY4mq8.png" alt=""/></p> <p>If you look at the above response, the first line shows a status code. This is quite important as it tells us the status of the response. There are five types of statuses detailed using a numerical code.</p> <p>● 1xx indicates an informational message only</p> <p>● 2xx indicates success of some kind</p> <p>● 3xx redirects the client to another URL</p> <p>● 4xx indicates an error on the client’s part</p> <p>● 5xx indicates an error on the server’s part</p> <p>So, if you encountered an error, you can take a look at the HTTP response to check what type of status code you have received. You can also use Postman to check the status of HTTP response.</p> <p><img src="https://cdn-images-1.medium.com/max/2864/0*cukuZ-3iIKY3-wXS.jpg" alt="Postman Learning Center"/></p> <h2 id="displaying-the-html-content">Displaying the HTML Content</h2> <p>The browser displays the HTML content in phases. First, it will render the bare-bone HTML skeleton. Then it will check the HTML tags and send out GET requests for additional elements on the web page, such as images, CSS stylesheets, JavaScript files, etc. These static files are cached by the browser, so it doesn’t have to fetch them again the next time you visit the page. In the end, you’ll see <a href="http://www.google.com">www.google.com</a> appearing on your browser. (Maneesa)</p> <p><img src="https://cdn-images-1.medium.com/max/2000/0*SmWIbPqyBZeZwais.jpg" alt="PCMag"/></p> <h2 id="how-we-can-use-cloud-services-such-as-awsgcpazure-terraform-kubernetes-and-docker-in-this-example">How we can use cloud services such as AWS/GCP/Azure, Terraform, Kubernetes, and Docker in this example?</h2> <p>Now let’s say you have built your own front-end with ReactJS/NextJS, pure JavaScript/HTML/CSS, etc. You want to deploy it on the Internet and get a domain so anyone can access your website online. I will keep these steps simple as our blog post is long already.</p> <p><img src="https://cdn-images-1.medium.com/max/2036/1*dW4xFlkCqpoaanQp9Msccw.png" alt=""/></p> <h2 id="step-1-server">Step 1: Server</h2> <p>For every front end, you need a server to run your website online. Luckily, cloud providers such as AWS/GCP/Azure all offer automatic scaling so you don’t have to worry about scaling your website. (We will talk more about scaling — specifically</p> <ul> <li> <p>vertical scaling (scaling up): increase the capacity of a single resource, such as a server or a virtual machine, by adding more power, memory, or storage to handle increased workload or demand</p> </li> <li> <p>horizontal scaling (scaling out): add more instances or copies of a resource, such as servers or virtual machines, to distribute the workload and increase the overall capacity</p> </li> <li> <p>load balancing: a technique used to distribute incoming network traffic across multiple servers or resources to ensure efficient utilization and improve performance.</p> </li> </ul> <p>in another blog). This is not to mention add-ons feature such as analytics, and storage, database offered by these cloud providers.</p> <p><img src="https://cdn-images-1.medium.com/max/3200/0*s6oHb5Xw7h8NE_o0.png" alt="K21Academy"/></p> <h2 id="step-2-docker-and-kubernetes">Step 2: Docker and Kubernetes</h2> <ul> <li> <p>Docker: Docker helps you pack everything you need to build your front-end site including libraries and settings into a single container.</p> </li> <li> <p>Kubernetes: Let’s say you have multiple containers, which hold your applications and distribute them across different computers called nodes. Kubernetes ensures that the containers are running correctly, monitors their health, and handles the situation if something goes wrong like crashing.</p> </li> </ul> <p>We use Docker and Kubernetes to package your application and deploy it on the server. It would require some DevOps skills to deploy a container onto the server.</p> <p><img src="https://cdn-images-1.medium.com/max/3200/0*YECr1A17-nSBwbu7.png" alt="Docker"/></p> <h3 id="more-on-devops">More on DevOps</h3> <p><img src="https://cdn-images-1.medium.com/max/2000/0*EBeWXRpf-N6UejFa.png" alt="Cl"/></p> <p>An example of DevOps CI/CD is when we push a commit to our desired Git and version control platform (Github, Gitlab, BitBucket, etc), which contains our</p> <ul> <li>Dockerfile (contains instructions to install and run the website like how we did with the front-end).</li> </ul> <p>Example of Dockerfile</p> <div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    <span class="c"># Use a base image</span>
    FROM python:3.9-alpine
    
    <span class="c"># Set the working directory</span>
    WORKDIR /app
    
    <span class="c"># Copy the requirements file</span>
    COPY requirements.txt .
    
    # Install dependencies
    RUN pip install --no-cache-dir -r requirements.txt
    
    <span class="c"># Copy the application code</span>
    COPY app.py .
    
    # Expose a port
    EXPOSE 8000
    
    <span class="c"># Set the command to run the application</span>
    CMD ["python", "app.py"]

</code></pre></div></div> <ul> <li>Kubernetes file (YAML) to run the Docker image.</li> </ul> <p>Example of Kubernetes file (YAML)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-app-container
          image: my-app-image:latest
          ports:
            - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: my-app-service
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: LoadBalancer
</code></pre></div></div> <p>We want for every commit pushed, the new version of the site will be deployed on the server automatically. Most cloud providers have their own CI/CD tool (AWS Code Pipeline, Google Cloud Build, Azure DevOps, etc.)</p> <p>We can configure the stages into these steps:</p> <ul> <li> <p>Source: Configure CodePipeline to monitor your GitHub repository as the source and pull the latest code changes.</p> </li> <li> <p>Build: Use a build stage (e.g., AWS CodeBuild) to build your Docker image based on the Dockerfile.</p> </li> <li> <p>Test: Include testing steps in your pipeline to verify the integrity and quality of your application.</p> </li> <li> <p>Deploy: Use a deployment stage to deploy the Docker image to your Kubernetes cluster. This stage will update the Kubernetes Deployment with the new image version.</p> </li> </ul> <p>Lastly, set up deployment triggers in your Kubernetes cluster to detect changes in the Deployment and automatically deploy the updated Docker image to your EC2 instance.</p> <h2 id="step-3-networking">Step 3: Networking</h2> <p>Once the website is deployed on the server, the server will return a public IP address of the server. You can also access the website via this IP address.</p> <h2 id="step-4-find-a-domain">Step 4: Find a domain</h2> <p>Buying a domain from sites such as CloudFlare, and Google Domain and connecting the IP address with the domain we just bought.</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*c9JqXmzDqdeXO4oSYHmyFQ.png" alt="CloudFlare"/></p> <p>That’s it! Now everyone can access your website with this new domain you just bought.</p> <h2 id="optional-terraform">Optional: Terraform</h2> <p>Most of the time, creating Cloud infrastructure such as servers, VPC, storage, etc., and connecting them all together as well as destroying them will take a lot of time. Terraform is an open-source infrastructure as code (IaC) that helps you manage and provision your cloud resources in simple a consistent way. We won’t go too much on Terraform today but with the configurations you have in Terraform, you can create and deploy those whole processes with just a few lines of commands.</p> <p>Note: You might need VPC (Virtual Private Cloud) when working with Cloud providers. A VPC allows you to create a logically isolated section of the cloud where you can launch and connect resources like virtual machines, databases, and containers.</p> <p><img src="https://cdn-images-1.medium.com/max/3200/0*5uyGXjF-LL3Y2E0d.png" alt="Terraform"/></p> <p>Example of Terraform code:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="s">provider "aws" {</span>
      <span class="s">region = "us-west-2"</span>
    <span class="s">}</span>
    
    <span class="s">resource "aws_instance" "example" {</span>
      <span class="s">ami           = "ami-0c94855ba95c71c99"</span>   <span class="c1"># Replace with your desired AMI ID</span>
      <span class="s">instance_type = "t2.micro"</span>
      <span class="s">key_name      = "your_key_pair_name"</span>      <span class="c1"># Replace with your key pair name</span>
    
      <span class="s">tags = {</span>
        <span class="s">Name = "example-instance"</span>
      <span class="s">}</span>
    <span class="s">}</span>
</code></pre></div></div> <p>Deploying your website is definitely a prolonged process. It makes me wonder how far we have come to have the websites loaded within seconds. I hope this article will help you understand more about the behind processes when we enter the URL into the address bar.</p> <p>Feel free to like, comment or share this article if it was helpful:)</p> <h2 id="references">References:</h2> <p>Wijesinghe-Nelken, Maneesa. “What Happens When You Type a URL in the Browser and Press Enter?” <em>Medium</em>, 3 Jan. 2020, medium.com/p/bb0aa2449c1a.</p>]]></content><author><name></name></author><category term="cloud"/><category term="cloud"/><category term="networking"/><summary type="html"><![CDATA[What happens when you type a URL into your browser? — The big picture (with Cloud)]]></summary></entry><entry><title type="html">Visualize your data from cloud with DoubleCloud and ClickHouse DB</title><link href="https://christopherle.com//blog/2023/clickhouse/" rel="alternate" type="text/html" title="Visualize your data from cloud with DoubleCloud and ClickHouse DB"/><published>2023-03-05T13:56:00+00:00</published><updated>2023-03-05T13:56:00+00:00</updated><id>https://christopherle.com//blog/2023/clickhouse</id><content type="html" xml:base="https://christopherle.com//blog/2023/clickhouse/"><![CDATA[<h2 id="visualize-your-data-from-cloud-with-doublecloud-and-clickhouse-db">Visualize your data from cloud with DoubleCloud and ClickHouse DB</h2> <p>Today, I will introduce to you a new platform I found for integrating your Cloud provider (AWS, GCP, Azure) and visualizing the data: DoubleCloud.</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*F7S-_NfF_4LWk14v-PbM-Q.png" alt="Visualization of DoubleCloud workflow"/></p> <p>For those who don’t know, DoubleCloud is a new platform that helps you build sub-second data analytical solutions and pipelines on proven open-source technologies like ClickHouse® and Apache Kafka®.</p> <p>The reason why I wrote about this tool was the same reason I found visualizing data from cloud providers automatically can be a bit hassle. Of course, there are other players in this function too like AWS QuickSight, Tableau, etc. However, I believe that this particular new tool offers unique features, and DB options, that make it worth considering for anyone who regularly works with data from cloud providers.</p> <p><img src="https://cdn-images-1.medium.com/max/3600/0*YUHFTfwuWOdphc_B.jpg" alt="DoubleCloud Logo"/></p> <p>For your inquiries:</p> <p><img src="https://cdn-images-1.medium.com/max/2800/0*n39rgloE4aHnh3B3" alt="ClickHouse"/></p> <blockquote> <p>ClickHouse is a powerful and high-performance database management system designed for real-time data processing and analysis. It was originally created by Yandex, a Russian search engine, to support their web analytics platform called Metrica. Compared to other database providers, ClickHouse is an open-source software that uses a columnar data model to store and retrieve data. This means that data is stored in columns rather than rows, which allows for faster query processing and data compression. ClickHouse is particularly well-suited for OLAP workloads, where complex queries are executed on large datasets. It supports SQL-like query language and allows for real-time data processing and analysis, making it ideal for applications that require fast and accurate insights from large volumes of data.</p> </blockquote> <p><img src="https://cdn-images-1.medium.com/max/2000/0*FgyBIh3kI-pue7Go.png" alt="Apache Kafka"/></p> <blockquote> <p>Apache Kafka is a software platform that facilitates the distributed processing of large volumes of data streams in real-time. It is an open-source system that allows companies to build high-performance data pipelines, real-time data processing systems, and streaming analytics applications at scale. Apache Kafka allows users to publish, subscribe to, store, and process streams of records in real-time. It is designed to handle large volumes of data efficiently, allowing users to process data as it arrives, without the need to wait for it to be fully collected or batched. One of the key benefits of Apache Kafka is its ability to handle large-scale data processing tasks across multiple nodes in a distributed environment. It provides high availability and fault tolerance, ensuring that data streams are processed consistently and reliably.</p> </blockquote> <p>In this blog, we will use ClickHouse as our main DB connection.</p> <h2 id="table-of-content">Table of content</h2> <ol> <li> <p>Preparing our dataset</p> </li> <li> <p>Create Cluster — Setting up DoubleCloud</p> </li> <li> <p>Transfer — Create a connection between your AWS S3 Database and ClickHouse</p> </li> <li> <p>Visualize your dataset</p> </li> </ol> <h2 id="preparing-our-dataset">Preparing our dataset</h2> <h3 id="getting-the-data">Getting the data.</h3> <p>In this blog, we will use the layoff dataset from my previous <a href="https://medium.com/@locvicvn1234/analysis-of-current-layoffs-in-the-usa-with-tableau-a8b1077a16b4">blog</a>. You can get it <a href="https://www.kaggle.com/datasets/swaptr/layoffs-2022">here</a> from Kaggle (FYI, you might need to log in to download the dataset). The dataset will be named “layoffs.csv”</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*3C_uxIJCNDb_SvMm6alQ8A.png" alt="What the layoffs dataset will look like"/></p> <h3 id="uploading-the-dataset-to-aws-s3-bucket">Uploading the dataset to AWS S3 Bucket</h3> <p>As of now, DoubleCloud offers connections with AWS so we will use AWS as our endpoint.</p> <p>To upload your dataset to AWS S3 Bucket, you need to:</p> <ol> <li> <p>Sign in to the AWS Management Console and open the Amazon S3 console at <a href="https://console.aws.amazon.com/s3/">https://console.aws.amazon.com/s3/</a>.</p> </li> <li> <p>In the left navigation pane, choose <strong>Buckets</strong>.</p> </li> <li> <p>Choose <strong>Create bucket</strong>.</p> </li> <li> <p>The <strong>Create bucket</strong> page opens.</p> </li> <li> <p>For <strong>Bucket name</strong>, enter a name for your bucket. For now, let’s name it <strong>doublecloudtest</strong></p> </li> </ol> <p><img src="https://cdn-images-1.medium.com/max/2180/1*KTezo2EaR5yRwiD6t0Nz-g.png" alt="Create AWS S3 bucket"/></p> <ol> <li>Once your bucket is created, upload your dataset “layoffs.csv” from your local computer.</li> </ol> <p><img src="https://cdn-images-1.medium.com/max/2000/1*jmT4AVYMjNLSSPzk-zZbXw.png" alt=""/></p> <ol> <li>The result should be like this.</li> </ol> <p><img src="https://cdn-images-1.medium.com/max/2594/1*1zRq14f-1ObbeA_ytZnK7Q.png" alt=""/></p> <p>More on creating AWS S3 Bucket from <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html">here</a>.</p> <p>Note that, this tutorial only focuses on csv file. In real life, you might configure for the real-time database.</p> <h2 id="setting-up-doublecloud">Setting up DoubleCloud</h2> <ol> <li>First, you need a Managed ClickHouse® cluster.</li> </ol> <ul> <li> <p>To do this, create or log in to your account. Go to <a href="https://auth.double.cloud/login?client_id=yc.oauth.doubleconsole&amp;redirectUrl=https%3A%2F%2Fauth.double.cloud%2Foauth%2Fauthorize%3Fresponse_type%3Dcode%26client_id%3Dyc.oauth.doubleconsole%26scope%3Dopenid%26redirect_uri%3Dhttps%253A%252F%252Fapp.double.cloud%252Fauth%252Fcallback%26state%3DzoCg90o7ZO5lqMfsOrJyGXwT7ASzFcMEr9s31ms">console</a>.</p> </li> <li> <p>Create a Cluster with ClickHouse service. Let’s name it “doublecloud1” for now</p> </li> <li> <p>At this moment, DoubleCloud is supporting AWS. We will choose AWS as our data source now.</p> </li> </ul> <p><img src="https://cdn-images-1.medium.com/max/2000/1*ZyEYkoVG4TgE4400DbgmYA.png" alt="Creating cluster with DoubleCloud"/></p> <ul> <li>For now, we will use default settings for our tutorial. Click submit to create a cluster. Once your cluster is done, the status will be Alive.</li> </ul> <p><img src="https://cdn-images-1.medium.com/max/2246/1*79YwG1q5XvSFVRZymZfGtA.png" alt=""/></p> <h2 id="create-a-database-on-the-newly-created-cluster">Create a database on the newly created cluster</h2> <ol> <li> <p>Install ClickHouse.</p> <h1 id="for-macos">For MacOS</h1> <p>curl https://clickhouse.com/ | sh</p> <h1 id="for-linux">For Linux</h1> <p>curl https://clickhouse.com/ | sh sudo ./clickhouse install</p> </li> <li> <p>Go to your Cluster detail on DoubleCloud, copy the native interface code, and run in the terminal. It should look like this:</p> <blockquote> <p>clickhouse-client — host <strong>**</strong><strong>**</strong><strong>**</strong><strong>**<em>.at.double.cloud — port **** — secure — user admin — password **</em></strong><strong>**</strong><strong>**</strong><strong>**</strong><strong>**</strong><strong>**</strong><strong>**</strong><strong>**</strong><strong>**</strong><strong>**</strong><em>**</em></p> </blockquote> </li> </ol> <p>Notice that you might change the code due to recent ClickHouse’s update.</p> <blockquote> <p>./clickhouse client — host <strong>**</strong><strong>**</strong><strong>**</strong><strong>**<em>.at.double.cloud — port **** — secure — user admin — password **</em></strong><strong>**</strong><strong>**</strong><strong>**</strong><strong>**</strong><strong>**</strong><strong>**</strong><strong>**</strong><strong>**</strong><strong>**</strong><em>**</em></p> </blockquote> <p><img src="https://cdn-images-1.medium.com/max/2146/1*J_NAMvneP15HP7IfHaoUzQ.png" alt="Cluster detail interface"/></p> <blockquote> <p>Note that you will need to save the <strong>user, **and **password</strong> information elsewhere for later usage in the transfer.</p> </blockquote> <ol> <li>Create a database from your terminal <blockquote> <p>create database [YOUR_DATABASE_NAME] on CLUSTER [DOUBLECLOUD_CLUSTER_NAME]</p> </blockquote> </li> </ol> <p>where:</p> <ul> <li> <p>YOUR_DATABASE_NAME: the name you want for your database. Let’s do it Sample-ClickHouse-DB for now</p> </li> <li> <p>DOUBLECLOUD_CLUSTER_NAME: The name of your DoubleCloud cluster, which is “TestCluster” for now.</p> </li> </ul> <h2 id="create-a-connection-between-your-aws-s3-database-and-clickhouse">Create a connection between your AWS S3 Database and ClickHouse</h2> <ol> <li> <p>Go to <strong>Transfer</strong>.</p> </li> <li> <p>Go to <strong>Endpoints.</strong></p> </li> </ol> <h3 id="for-source">For source:</h3> <ol> <li> <p>Click <strong>Create endpoint</strong>, choose <strong>Source</strong></p> </li> <li> <p>Choose <strong>AWS S3</strong> as your source</p> </li> <li> <p>Choose a name of your choice. For now, let’s name it <strong>s3-source-quickstart</strong></p> </li> <li> <p>Configure your endpoint parameters:</p> </li> </ol> <ul> <li> <p>Dataset is <strong>layoffs</strong></p> </li> <li> <p>path pattern is <strong>*csv</strong></p> </li> </ul> <p><img src="https://cdn-images-1.medium.com/max/2000/1*5yl2zfenjgInxjmN_sXNzA.png" alt=""/></p> <ol> <li> <p>In the <strong>S3: Amazon Web Services</strong>, enter your bucket name, AWS Access Key ID, and AWS Secret Access Key.</p> </li> <li> <p>Submit.</p> </li> </ol> <p><img src="https://cdn-images-1.medium.com/max/2000/1*ofJWRQXWHQ68U2XhL8EQcg.png" alt=""/></p> <h3 id="to-get-your-aws-access-key-id-and-secret-access-key">To get your AWS access key ID and secret access key</h3> <ol> <li> <p>Open the IAM console at <a href="https://console.aws.amazon.com/iam/">https://console.aws.amazon.com/iam/</a>.</p> </li> <li> <p>On the navigation menu, choose <strong>Users</strong>.</p> </li> <li> <p>Choose your IAM user name (not the check box).</p> </li> <li> <p>Open the <strong>Security credentials</strong> tab, and then choose <strong>Create access key</strong>.</p> </li> <li> <p>To see the new access key, choose <strong>Show</strong>. Your credentials resemble the following:</p> </li> </ol> <ul> <li> <p>Access key ID: AKIAIOSFODNN7EXAMPLE</p> </li> <li> <p>Secret access key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY</p> </li> </ul> <ol> <li>To download the key pair, choose <strong>Download .csv file</strong>. Store the .csv file with keys in a secure location.</li> </ol> <h3 id="for-target">For target</h3> <ol> <li> <p>Click <strong>Create endpoint</strong>, choose <strong>Target</strong></p> </li> <li> <p>Choose <strong>ClickHouse</strong> as your target</p> </li> <li> <p>Choose a name of your choice. For now, let’s name it <strong>clickhouse-target-quickstart</strong></p> </li> <li> <p>Configure your endpoint parameters:</p> </li> </ol> <ul> <li> <p>Database is <strong>Sample-ClickHouse-DB</strong></p> </li> <li> <p>Pass your user and password from your Cluster information.</p> </li> </ul> <p><img src="https://cdn-images-1.medium.com/max/2000/1*sdhUtfZWExJVMO_yjGiVaA.png" alt=""/></p> <ol> <li> <p>Leave the rest for the default settings.</p> </li> <li> <p>Submit.</p> </li> </ol> <h3 id="test-the-connection">Test the connection</h3> <p>Once source and target endpoints are created, go back to the transfers tab and choose <strong>Activate.</strong></p> <p><img src="https://cdn-images-1.medium.com/max/2170/1*QnTNVVjiAUGqDc9mmcX6xQ.png" alt=""/></p> <p>If the status is <strong>Done</strong>, the connection is successful.</p> <p>If the status is <strong>Error</strong> with the red color, click on the Error, then <strong>Logs</strong>, to see the error message. Usually, double-check if you already entered all the correct credentials from ClickHouse and AWS.</p> <h2 id="visualize-your-dataset">Visualize your dataset</h2> <ol> <li> <p>Go to <strong>Visualization</strong>.</p> </li> <li> <p>Create a new <strong>workbook</strong>. Let’s name it Demo Workbook.</p> </li> <li> <p>Create a new <strong>connection</strong>. Choose <strong>ClickHouse.</strong> The interface will ask for Hostname, Username and Password.</p> </li> <li> <p>Go to your cluster from Clusters, and go to <strong>Hosts</strong> to obtain host information</p> </li> </ol> <p><img src="https://cdn-images-1.medium.com/max/2116/1*uUUdIhTNY7buxLO6JRdu_Q.png" alt=""/></p> <ol> <li>Copy host, username, and password in the Connection.</li> </ol> <p><img src="https://cdn-images-1.medium.com/max/2000/1*c9i16hS1Xjg1ZmuEsqCF3Q.png" alt=""/></p> <ol> <li> <p><strong>Check connection</strong>. If there is a green tick then <strong>Create connection.</strong></p> </li> <li> <p>Create dataset. Select the layoffs dataset.</p> </li> </ol> <p>You might see the column is combined as a string instead of separate columns.</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*2FUvjYr0AvgMPAjgp3DTcA.png" alt=""/></p> <p>While this is a small issue, to handle this, we create a new <strong>field</strong>. Let’s name it the <strong>company</strong> for example. There, we split the combined string with “,” and select the first index returned from the split array, which is the company name. Choose <strong>formula</strong>, then enter this below</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SPLIT([company,location,industry,total_laid_off,percentage_laid_off,date,stage,country,funds_raised], ",", 1)
</code></pre></div></div> <p><img src="https://cdn-images-1.medium.com/max/2000/1*QgXt-H-y_EJ9fuRrpNMQ4A.png" alt="After spliting company name, the result will be like this"/></p> <p>Click <strong>save</strong>. Do the same for other columns you want to add. Remember to change to the corresponding type (string, integer, fractional number, etc).</p> <p>For now, let’s create 2 new fields with <strong>industry (index 3)</strong>, and <strong>total_laid_off (index 4)</strong> for simple visualization.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// industry
SPLIT([company,location,industry,total_laid_off,percentage_laid_off,date,stage,country,funds_raised], ",", 3)

// total_laid_off
SPLIT([company,location,industry,total_laid_off,percentage_laid_off,date,stage,country,funds_raised], ",", 4)
</code></pre></div></div> <ol> <li> <p>Once the database preparation is done, click <strong>create chart</strong></p> </li> <li> <p>Drop <strong>total_laid_off in Y</strong>, and <strong>industry in X.</strong> The result will be as below.</p> </li> </ol> <p><img src="https://cdn-images-1.medium.com/max/3548/1*E9b949iDj6MaD5gaL1v9ww.png" alt=""/></p> <h2 id="conclusion">Conclusion</h2> <p>If you reach this far, congratulations! You just learned to visualize your data from the cloud provider (AWS S3) with DoubleCloud and ClickHouse.</p> <p>My thought on this new platform is that it is very simple to use: the visualization tool with drag and drop reminds me of Tableau. Meanwhile, the simple experience to connect with data from a cloud provider with no code is impressive. Really looking forward to seeing how DoubleCloud goes in the future with more Cloud options from GCP and Azure.</p> <h2 id="references">References</h2> <iframe src="https://medium.com/media/e72cf162a23a457b10900eea98aa9093" frameborder="0"></iframe> <ul> <li> <p>AWS S3 Documentation</p> </li> <li> <p>AWS IAM Documentation</p> </li> <li> <p>DoubleCloud Documentation</p> </li> <li> <p>ClickHouse Documentation</p> </li> </ul>]]></content><author><name></name></author><category term="demo"/><category term="cloud"/><category term="demo"/><summary type="html"><![CDATA[Visualize your data from cloud with DoubleCloud and ClickHouse DB]]></summary></entry><entry><title type="html">Analysis of Current Layoffs in the USA with Tableau</title><link href="https://christopherle.com//blog/2022/tableu-layoffs/" rel="alternate" type="text/html" title="Analysis of Current Layoffs in the USA with Tableau"/><published>2022-11-21T14:56:00+00:00</published><updated>2022-11-21T14:56:00+00:00</updated><id>https://christopherle.com//blog/2022/tableu-layoffs</id><content type="html" xml:base="https://christopherle.com//blog/2022/tableu-layoffs/"><![CDATA[<h2 id="analysis-of-current-layoffs-in-the-usa-with-tableau">Analysis of Current Layoffs in the USA with Tableau</h2> <p>When you thought the pandemic was over, the year 2022 hit with a recession with most tech giants doing layoffs. Today’s post will be some of the insights I gained when doing personal exploratory data analysis on the tech layoff dataset. This analysis aims to have some insights into the current layoff situation and market. Therefore, we can somehow learn the cause, and what to prepare for in the future.</p> <p>Most of the visualizations are done by Tableau with data taken from <a href="https://www.kaggle.com/datasets/swaptr/layoffs-2022">Kaggle</a>, updated on Nov 21, 2022. These visualizations will be backed up with personal assumptions. Note that these personal observations came from my self-learning with Tableau. Please make any necessary suggestions if you think these can be improved.</p> <p>First, let’s look at the tech industries that were laid off in 2022. We can see the top three were retail, consumer, and transportation. I assumed that with the ongoing recession, people were more aware of controlling their consumption and transportation. Not to mention, the pandemic with remote work has motivated people to stay at home more often.</p> <p><img src="https://cdn-images-1.medium.com/max/4184/1*hCHuS5Wr9QTvpNSTxdzaiQ.png" alt=""/></p> <p>Compared to previous years, when you look at the year 2020 when the pandemic first happened, the top laid-off industries were transportation, travel, and finance. In 2021, these industries tended to have fewer layoffs. I assumed that people got used to the pandemic in 2021 and companies started to bounce back.</p> <p><img src="https://cdn-images-1.medium.com/max/4140/1*Dpo_BcpHVLk2kEuq0ffRbA.png" alt=""/></p> <p>Let’s have a look at the monthly trend from 2020 to 2022 to see what happened.</p> <p><img src="https://cdn-images-1.medium.com/max/3324/1*wHIbmy1qzQCbf-LnJIou1g.png" alt="Retail and consumer are the top 2 industries with the most laid-offs in 2022"/></p> <p>Those companies that laid off the most in 2022 were those at the IPO stage, followed by unknown, series C, and series B.</p> <p><img src="https://cdn-images-1.medium.com/max/4184/1*wUDUxieoXSSYL2JKZ84hYg.png" alt=""/></p> <p>Most laid-offs happened in the cities, especially in the West (SF Bay Area, Seattle, Los Angles). This contributes to the fact that most tech companies are located in the West.</p> <p><img src="https://cdn-images-1.medium.com/max/4184/1*cbZwcCDqmktRMvRSwE-fsw.png" alt=""/></p> <p>Undoubtedly, the top layoffs came from Meta and Amazon with 11,000 and 10,000 layoffs this year.</p> <p><img src="https://cdn-images-1.medium.com/max/4120/1*6R20lPdW3Fw-0k8AjQ4-Cw.png" alt=""/></p> <p>In terms of the funding raised in 2022, the media industry was the highest with a low laid-off value, with the major of funding being for Netflix. My assumption was that with the current pandemic, more and more people stayed at home and spent more time on movies, and streaming services.</p> <p><img src="https://cdn-images-1.medium.com/max/4184/1*KVc145_etG6COGrtbWWndA.png" alt=""/></p> <p>Let’s put funds raised and total laid-off in 2022 on a scatterplot to see their relationship. We can generally see that until Meta with a total fund raised of 26,000 million dollars, the higher the fund raised, the more company will lay off. After 26,000 million USD, it was hard to tell the relationship between funds raised and total laid-off.</p> <p><img src="https://cdn-images-1.medium.com/max/2824/1*fexfq15dN5s5MudlDxqkAg.png" alt=""/></p> <p>What I did, in addition, was to add clustering and averaging line with 95% CI to see the trend. The resulting trend was below. While it was hard to make sense of these groups (only the future will tell), I will leave this part for the readers to do a self-assessment.</p> <p><img src="https://cdn-images-1.medium.com/max/2360/1*5XTpdnWNiIhcZ_mPXbs8zA.png" alt=""/></p> <h2 id="what-i-learned-from-the-recession">What I learned from the recession</h2> <p>Remember last year, there were so many LinkedIn posts of people getting six-figure new grad offers from big tech companies. I assume that these companies were aiming for a come-back after the pandemic, therefore hiring as many engineers as possible to prepare for post-pandemic growth. However, that growth did not come with the unexpected recession. One of the best ways companies can right now survive is: to lay off people and focus on areas that show growth.</p> <p>With the massive layoffs from big tech, smaller companies will have more chances to attract talents with affordable costs, especially those who are under visa restriction. In other words, new grads in 2022 (and maybe 2023) will be likely to face great competition to get a job in tech.</p> <p>What I observed was that the world somehow always comes back to its balance and nothing is stable forever. The year 2021 came with massive tech hiring with tremendously high salaries and great visa support in STEM majors. This made more people want a career in tech. Therefore, finding and keeping a tech career is no longer easy. With more and more people learning coding interview techniques to get to companies, getting to top companies is no longer something impossible. Certainly, it is not easy to get there.</p> <p>What one can do is practice coding interviews frequently to prepare for any unexpected. After all, you cannot control external factors; You cannot predict the future nor you cannot change the past. It matters how would you react to the current situation.</p> <p><strong>So what can you do now?</strong></p> <ul> <li> <p>Control your budget, spending, and saving to prepare for what is coming next.</p> </li> <li> <p>Do your best at your current job to prevent being laid off. Also, remember to showcase your work/performance.</p> </li> <li> <p>However, practice coding interviews to prepare for a job search in case the uncontrollable happens.</p> </li> <li> <p>Review what you actually want (job with high salary, job that you like, or take some gap year for mental break, etc)</p> </li> <li> <p>Pick the jobs/industries/companies that are recession-proof. One way, in my opinion, is to see Maslow and the supply chain. What people always need first are physical and safety needs, meaning that you should pick the tech industries that are related to those categories. They may not give you a huge package but should you have some job security in mind first. Again, company research is needed.</p> </li> </ul> <p><img src="https://cdn-images-1.medium.com/max/2000/0*vL9x1MTy0fI7Rd-O" alt="Maslow and the supply chain"/></p> <h2 id="final-thoughts">Final thoughts</h2> <p>Let’s be honest. Job search is not easy. I remember last year, even before the recession, I found my first internship after +150 rejections. Although the recession may seem pretty bad for the tech job market now, especially for those under visa restrictions, it is also a good time for one to reflect on their values, practice coding interviews, control their budget wisely, or even spend more time with their loved ones. I do believe that at the end of the road, there will be a way. Keep looking! You got this!</p>]]></content><author><name></name></author><category term="AI"/><category term="tableau"/><category term="analytics"/><summary type="html"><![CDATA[Analysis of Current Layoffs in the USA with Tableau]]></summary></entry><entry><title type="html">Current reading books</title><link href="https://christopherle.com//blog/2022/reading/" rel="alternate" type="text/html" title="Current reading books"/><published>2022-11-21T13:56:00+00:00</published><updated>2022-11-21T13:56:00+00:00</updated><id>https://christopherle.com//blog/2022/reading</id><content type="html" xml:base="https://christopherle.com//blog/2022/reading/"><![CDATA[<ul> <li>System Design Interview pt1&amp;2</li> <li>Design Intensive Data Application</li> <li>SRE practices at Google</li> </ul>]]></content><author><name></name></author><category term="reading"/><category term="tech"/><summary type="html"><![CDATA[Current reading books]]></summary></entry><entry><title type="html">Almost all about that graph (part 1)</title><link href="https://christopherle.com//blog/2020/all-about-that-graph/" rel="alternate" type="text/html" title="Almost all about that graph (part 1)"/><published>2020-05-07T13:56:00+00:00</published><updated>2020-05-07T13:56:00+00:00</updated><id>https://christopherle.com//blog/2020/all-about-that-graph</id><content type="html" xml:base="https://christopherle.com//blog/2020/all-about-that-graph/"><![CDATA[<h2 id="algorithm-almost-all-about-that-graph-part-1">[Algorithm] Almost all about that graph (part 1)</h2> <h2 id="table-of-content">Table of content:</h2> <blockquote> <p><strong><em>I. Why graph</em></strong> <strong><em>II. What is graph</em></strong> <strong><em>III. Types of graph algorithms:</em></strong></p> </blockquote> <p>Part 1:</p> <blockquote> <p><em>1. Graph traversal (BFS, DFS)</em> <em>2. Finding the path with the lowest cost or shortest path</em> (besides BFS) <em>(Bellman-Ford, Dijkstra, Floyd-Warshall)</em></p> </blockquote> <p>Part 2:</p> <blockquote> <p><em>3. Minimum spanning tree (Prim, Union-Find)</em> <em>4. Sorting (Topological sort)</em> <strong><em>III. Example with a Leetcode problem</em></strong></p> </blockquote> <p>Part 3:</p> <blockquote> <p><strong><em>IV. Application in neural network — CNN (Building an image classification — Bonus topic)</em></strong> <strong><em>V. Conclusion</em></strong></p> </blockquote> <h2 id="i-why-graph">I. Why graph?</h2> <p>If you look closely at your smart phone’s applications or many other technical applications, the graph actually appears in every corner of our lives. From finding the shortest path from a location to another on Google Maps to graphing your connections on social media.</p> <p><img src="https://cdn-images-1.medium.com/max/2400/1*uCzI0jb4kaodm8uCiCmBng.jpeg" alt=""/></p> <p>Finding an optimal graph algorithm will reduce a great amount of cost or compute resources when applying graph with real life’s great amount of data like millions of real world’s streets (example of finding the shortest path on Google Map). (Well, unless you are trying to calculate the shortest distance between you and your crush’s heart, can’t help you with that 😅)</p> <h2 id="ii-what-is-a-graph-in-data-structure-and-algorithm">II. What is a graph (in data structure and algorithm)?</h2> <p><img src="https://cdn-images-1.medium.com/max/2000/1*_ZLmV0IH7_j8eQUrlG76hg.png" alt=""/></p> <ol> <li> <p>A graph is a set of vertices and a collection of edges that each connect a pair of vertices. (Sedgewick, Algorithms 518)</p> </li> <li> <p>There are two types of graph</p> </li> </ol> <p><img src="https://cdn-images-1.medium.com/max/2066/1*6JnpO83tjoWVaiAeenfUmQ.png" alt=""/></p> <ol> <li>Loop and Multiple edges</li> </ol> <p><img src="https://cdn-images-1.medium.com/max/2000/1*m2YDWOeLmjWrKl3xI7R5Ug.png" alt=""/></p> <p>A <strong>loop</strong> is an edge that connects a vertex to itself. If a graph has more than one edge joining some pair of vertices then these edges are called <strong>multiple edges</strong>.</p> <ol> <li>Simple Graph</li> </ol> <p><img src="https://cdn-images-1.medium.com/max/2000/1*eZ6bBNttU8etFZ2m0Vgo1Q.jpeg" alt=""/></p> <p>A <strong>simple graph</strong> is a graph that does not have more than one edge between any two vertices and no edge starts and ends at the same vertex. In other words a simple graph is a graph without loops and multiple edges.</p> <ol> <li>Degree of a Vertex</li> </ol> <p><img src="https://cdn-images-1.medium.com/max/2560/1*0GmSMV_yuzTKPDnnk3KLJQ.jpeg" alt=""/></p> <p>6.Path</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*w01pdHWNDujdZoIRFh-Pfg.jpeg" alt=""/></p> <p>A <strong>path</strong> is a sequence of vertices with the property that each vertex in the sequence is adjacent to the vertex next to it. A path that does not repeat vertices is called a <strong>simple path</strong>.</p> <ol> <li>A Connected Graph</li> </ol> <p><img src="https://cdn-images-1.medium.com/max/2000/1*8aIjeWLKSAPqxg3khYTVlQ.jpeg" alt=""/></p> <p>A graph is said to be <strong>connected</strong> if <strong>any two</strong> of its vertices are joined by a path. A graph that is not connected is a <strong>disconnected graph</strong>. A disconnected graph is made up of connected subgraphs that are called <strong>components</strong></p> <ol> <li>Weight:</li> </ol> <p><img src="https://cdn-images-1.medium.com/max/2000/1*6R5gBfMBbqZb3J1SU0c28w.gif" alt=""/></p> <p>In a weighted graph, each vertex is assigned with a numerical value</p> <ol> <li>Adjacent matrix and Edge list representation:</li> </ol> <p><img src="https://cdn-images-1.medium.com/max/2000/1*UZcutROLuyQtYnOlrSVAlw.png" alt=""/></p> <h2 id="iii-types-of-graph-algorithm">III. Types of graph algorithm:</h2> <h3 id="1-graph-traversal"><strong>1. Graph traversal:</strong></h3> <p>Essential — BFS (breadth-first search) and DFS (depth-first-search)</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*GT9oSo0agIeIj6nTg3jFEA.gif" alt=""/></p> <p>DFS: begin from starting vertex to ending vertex, and repeat until all vertexes are covered.</p> <p>BFS: begin from starting vertex to all vertexes but level by level.</p> <p><strong>a. Breadth-first search:</strong></p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*KAZbkOGxRrmTokzX6af2vA.gif" alt=""/></p> <p>Description: BFS is used to find the shortest path from a vertex to another (these vertexes should be connected) or path from 1 vertex to other vertexes in a graph.</p> <p>A great BFS demo by Sadanand Pai: <a href="https://sadanandpai.github.io/shortestpathfinder/">https://sadanandpai.github.io/shortestpathfinder/</a></p> <p>Detailed explanation: <a href="https://www.educative.io/edpresso/how-to-implement-a-breadth-first-search-in-python">https://www.educative.io/edpresso/how-to-implement-a-breadth-first-search-in-python</a></p> <p>Type of graph: undirected, directed, do not have weights (if not, weights are equal)</p> <p>Algorithmic use cases: finding the shortest path between two nodes, testing if a graph is bipartite, finding all connected components in a graph, etc.</p> <p>Complexity: O(V+E) — where V is the number of vertexes and E is the number of edges.</p> <p><strong>Code:</strong></p> <p>i. Implementing BFS:</p> <p><img src="https://cdn-images-1.medium.com/max/2860/1*-lsvcalsIgYjWi9SgPh-bA.png" alt=""/></p> <p>ii. Print path (2 methods):</p> <p><img src="https://cdn-images-1.medium.com/max/2720/1*Efb0roSG_ys7HU2IPDIuoA.png" alt=""/></p> <p>iii. Example usage:</p> <p><img src="https://cdn-images-1.medium.com/max/2720/1*gTiwg6QRb6ENcjOvmUOKmg.png" alt=""/></p> <p><strong>b. Depth-first search:</strong></p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*qKBL47VFOJs4loXu1rVUDA.png" alt=""/></p> <p>Description: DFS is used to find a path from a vertex to another (these vertexes should be connected). DFS solution is not necessarily the shortest path.</p> <p>Detailed explanation: <a href="https://www.educative.io/edpresso/how-to-implement-depth-first-search-in-python">https://www.educative.io/edpresso/how-to-implement-depth-first-search-in-python</a></p> <p>Type of graph: undirected, directed</p> <p>Algorithmic use cases: topological sorting, solving problems that require graph backtracking, detecting cycles in a graph, finding paths between two nodes, finding an exit of a maze, etc.</p> <p>Complexity: O(V+E)</p> <p><strong>Code:</strong></p> <p>i. Implementation:</p> <p><img src="https://cdn-images-1.medium.com/max/2720/1*g5jpogk2frllkaLWX1a04g.png" alt=""/></p> <p>ii. Example usage:</p> <p><img src="https://cdn-images-1.medium.com/max/2720/1*G2mt-kGla_esyV7CslQEBg.png" alt=""/></p> <h3 id="2-finding-the-path-with-the-lowest-cost-or-shortest-path-besides-bfs"><strong>2. Finding the path with the lowest cost or shortest path (besides BFS):</strong></h3> <p><em>In this chapter, for each edge, there will be a weight (cost) between two vertexes.</em></p> <p>a. Bellman-Ford</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*SdJ4ZUD4XuiJEqORTJn2GQ.gif" alt=""/></p> <p>Description: [Single-source] To find the path with the <strong>lowest cost from one vertex to every other vertex. Can be used to detect negative-weight cycle</strong></p> <p>Detailed explanation: <a href="https://www.geeksforgeeks.org/bellman-ford-algorithm-simple-implementation/">https://www.geeksforgeeks.org/bellman-ford-algorithm-simple-implementation/</a></p> <p>Type of graph: undirected, directed, have weights, <strong>weights can be positive or negative</strong></p> <p>Complexity in this implementation: O(V.E)</p> <p><strong>Code:</strong></p> <p>i. Implementation:</p> <p><img src="https://cdn-images-1.medium.com/max/2720/1*tbvKRVJhFY3TEAt9cFWvpA.png" alt=""/></p> <p>ii. Example usage:</p> <p><img src="https://cdn-images-1.medium.com/max/2720/1*-ngNUkyehk0CndENE2pMjA.png" alt=""/></p> <p>b. Dijkstra</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*15KkonMRnHdbzGhFw0PXCA.gif" alt=""/></p> <p>Description: [Single-source] Dijkstra is used to <strong>finding a path with the lowest cost from one vertex to every other vertex.</strong></p> <p>Detailed explanation: <a href="https://www.bogotobogo.com/python/python_Dijkstras_Shortest_Path_Algorithm.php">https://www.bogotobogo.com/python/python_Dijkstras_Shortest_Path_Algorithm.php</a></p> <p>Type of graph: undirected, directed, have <strong>positive weights</strong></p> <p>Complexity: O(V²) — optimal: O(ElogV) with a heap (priority queue)</p> <p><strong>Code:</strong></p> <p><img src="https://cdn-images-1.medium.com/max/2720/1*dv3iuAL-dDWDgnFlBdQS7w.png" alt=""/></p> <p>c. Floyd-Warshall</p> <p><img src="https://cdn-images-1.medium.com/max/2000/1*CAa3Bt9rB_l5p8wjm_5puA.png" alt=""/></p> <p>Description: [All-pairs] Floyd-Warshall is used to finding the shortest path for all pairs of vertexes.</p> <p>Detailed explanation: <a href="https://www.programiz.com/dsa/floyd-warshall-algorithm">https://www.programiz.com/dsa/floyd-warshall-algorithm</a></p> <p>Brief explanation:</p> <blockquote> <p><em>Floyd-Warshall uses Dynamic Programming method to save initial results into an array. Then it will run a loop again to check middle vertexes and their cost between themselves and starting and ending vertex.</em> <em>If current cost &gt; (new cost 1 (start tonew vertex) + new cost 2 (new vertex to end))</em> <em>then the current cost will update itself with (new cost 1 + new cost 2)</em></p> </blockquote> <p>Type of graph: <strong>directed only, weights can be negative or positive, cannot be used in the negative cycle</strong></p> <p>Complexity: O(V³)</p> <p>Code:</p> <p>i. Implementation</p> <p><img src="https://cdn-images-1.medium.com/max/2720/1*00tbXgW3FYSk3AeTDTxkTw.png" alt=""/></p> <p>ii. Sample usage:</p> <p><img src="https://cdn-images-1.medium.com/max/3048/1*jcgP44jiNPWi0KYusf1Nig.png" alt=""/></p> <h2 id="summary">Summary:</h2> <p>Hope that this blog can help you understand more about graph algorithms and when to use them specifically.</p> <p>Here is the overview. Stay tuned for part 2.</p> <p>For better visualization, I strongly recommend this site: <a href="https://visualgo.net/en">https://visualgo.net/en</a></p> <p><img src="https://cdn-images-1.medium.com/max/3150/1*8O_GeY8kyb1M49NLJmOXBg.png" alt=""/></p> <h2 id="reference">Reference:</h2> <ol> <li> <p>Sedgewick, R., Wayne, K. (2011). <em>Algorithms, 4th Edition</em>. Addison-Wesley. ISBN: 978–0–321–57351–3</p> </li> <li> <p>CS 97SI: Introduction to Programming Contests from Stanford University</p> </li> <li> <p>Big-O Coding Algorithm Class</p> </li> </ol> <p>For any questions or concerns, please contact me at locvicvn1234@gmail.com</p> <p>My LinkedIn: <a href="https://www.linkedin.com/in/chrislevn/">https://www.linkedin.com/in/chrislevn/</a></p> <p>My Coding Challenges: <a href="https://github.com/chrislevn/Coding-Challenges">https://github.com/chrislevn/Coding-Challenges</a></p>]]></content><author><name></name></author><category term="tech"/><category term="algorithm"/><summary type="html"><![CDATA[[Algorithm] Almost all about that graph (part 1)]]></summary></entry></feed>